# Main Takeaway

了解一下神经网络的量化

<!--more-->

# 量化原因

同类模型中仍然是更大的网络，更好的效果，随着模型预测（predication）越来越准确，网络越来越深，神经网络消耗的内存大小和内存带宽成为问题，尤其是在移动设备上。因此有两种研究方向

1. 设计更有效的网络架构，用相对较小的模型尺寸达到可接受准确度，例如 MobileNet 和 SequeezeNet
2. 通过压缩、编码等方式减小网络规模。**量化**是最广泛采用的压缩方法之一。

# 什么是量化

量化就是将神经网络的浮点算法转换为定点。这可以在移动手机上实现网络的实时运行，对云计算的部署也有帮助。

## 学术界的工作

量化有若干相似的术语。低精度（Lowprecision）可能是最通用的概念。精度的区分

- 常规精度一般使用 FP32（32位浮点，单精度）存储模型权重；

- 低精度则表示FP16（半精度浮点），INT8+（8位的定点整数）等等数值格式。

  不过目前低精度往往指代INT8。

- 混合精度（Mixed precision）在模型中使用FP32和 FP16。

  FP16 减少了一半的内存大小，但有些参数或操作符必须采用FP32格式才能保持准确度。

量化一般指**INT8**。不过，根据存储一个权重元素所需的位数，还可以包括:

- 二值神经网络：在运行时权重和激活只取两种值（例如+1，-1）的神经网络，以及在训练时计算参数的梯度。
- 三元权重网络：权重约束为+1,0和-1的神经网络。
- XNOR网络：过滤器和卷积层的输入是二进制的。XNOR 网络主要使用二进制运算来近似卷积。

其他一些研究更关注如何压缩整个模型而非存储一个元素的位数。DeepCompression是该方向最重要的工作之一，作者将剪枝、量化和编码等技术结合起来

## 工业界的工作

工业界最终选择了INT8量化一一FP32 在推理（inference）期间被INT8取代，而训练(training）仍然是 FP32。TensorRT，TensorFlow，PyTorch，MxNet 和许多其他深度学习软件都已启用（或正在启用）量化。

通常，可以根据FP32 和INT8的转换机制对解决方案进行分类。一些框架简单地引I入了
Quantize和Dequantize层，当从卷积或全链接层送入或取出时，它将 FP32 转换为INT8 或相反。

# 量化的计算

量化过程可以分为两部分：将模型从 FP32 转换为 INT8，以及使用 INT8 进行推理。本节说明这两部分背后的算术原理。如果不了解基础算术原理，在考虑量化细节时通常会感到困惑。

先介绍一下定点与浮点：

- 定点保留特定位数整数和小数
- 浮点保留特定位数的有效数字（significand）和指数（exponent）

其它见link

# References

- [神经网络量化简介 | 黎明灰烬 博客 (zhenhuaw.me)](https://zhenhuaw.me/blog/2019/neural-network-quantization-introduction-chn.html)
- [(29 封私信 / 80 条消息) 神经网络量化简介](https://zhuanlan.zhihu.com/p/64744154)
- [Neural Network Quantization Introduction | 黎明灰烬 博客 (zhenhuaw.me)](https://zhenhuaw.me/blog/2019/neural-network-quantization-introduction.html)
