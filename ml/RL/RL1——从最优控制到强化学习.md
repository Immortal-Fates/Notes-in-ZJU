# Main Takeaway

我们先来从本质上理解以下强化学习：从最优控制到强化学习

<!--more-->

# 最优控制（Optimal Control）核心脉络

- 基本目标

  在动态系统约束下，寻找最优控制策略 $  u^*(t)  $，使性能指标 $  J  $ 极小化：
  $$
  J = \phi[x(t_f), t_f] + \int_{t_0}^{t_f} L[x(t), u(t), t] \, dt
  $$

- 核心方法

  | 方法                   | 应用场景       | 关键方程                                                    |
  | ---------------------- | -------------- | ----------------------------------------------------------- |
  | 变分法                 | 无约束开环控制 | 欧拉-拉格朗日方程                                           |
  | 极小值原理(Pontryagin) | 含约束系统     | 哈密顿系统：<br>$ \dot{x}=f, \dot{\lambda}=-∂H/∂x $         |
  | 动态规划               | 离散时间系统   | 贝尔曼方程：<br>$ V^*(s) = \min_u \sum r + \gamma V^*(s') $ |

- 关键瓶颈：**"维数灾"（Curse of Dimensionality）**→ 高维状态空间下价值函数计算复杂度指数级增长

# 强化学习的继承与突破

强化学习本质是一种自适应的最优控制

- 理论锚点：直接继承动态规划的贝尔曼方程框架

- 三大创新方向

  | 方向         | 核心思想                              | 代表算法   |
  | ------------ | ------------------------------------- | ---------- |
  | 无模型化     | 避开系统建模，通过试错学习            | Q-Learning |
  | 高维函数逼近 | 用神经网络拟合价值函数                | DQN, PPO   |
  | 策略梯度     | 直接优化随机策略 $ \pi_\theta(a\|s) $ | REINFORCE  |

- 交叉领域关键技术映射

  | 最优控制             | 强化学习             | 工业场景实例            |
  | -------------------- | -------------------- | ----------------------- |
  | 线性二次调节器 (LQR) | 策略梯度 (PG)        | 机器人平衡控制          |
  | 模型预测控制 (MPC)   | 基于模型的 RL (MBRL) | 自动驾驶轨迹规划        |
  | 哈密顿-雅可比方程    | 神经ODE + RL         | 航天器轨道优化          |
  | 离散时间动态规划     | 深度Q网络 (DQN)      | 游戏AI决策（如AlphaGo） |

# Conclusion

强化学习本质是**最优控制理论在复杂系统（高维/不确定/无模型）中的工程化实现**。
从庞特里亚金最大值原理到PPO算法，二者的理论同源性大于差异性——**贝尔曼方程始终是串联控制理论与学习的核心数学纽带**。

接下来准备看看Pieter Abbeel

# References

- 【从最优控制到强化学习】<https://www.bilibili.com/video/BV1MDWPzMEnw?vd_source=93bb338120537438ee9180881deab9c1>
