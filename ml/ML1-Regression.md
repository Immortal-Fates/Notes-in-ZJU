# Main Takeaway

å›å½’æ¨¡å‹

<!--more-->

æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„å…³é”®è¦ç´ æ˜¯è®­ç»ƒæ•°æ®ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–ç®—æ³•ï¼Œè¿˜æœ‰æ¨¡å‹æœ¬èº«

# Linear-Regression

## Mathematica

$$
\hat{y} = w_1  x_1 + ... + w_d  x_d + b.
$$

å°†æ‰€æœ‰ç‰¹å¾æ”¾åˆ°å‘é‡$\mathbf{x} \in \mathbb{R}^d$ä¸­ï¼Œå¹¶å°†æ‰€æœ‰æƒé‡æ”¾åˆ°å‘é‡$\mathbf{w} \in \mathbb{R}^d$ä¸­ï¼Œ

æˆ‘ä»¬å¯ä»¥ç”¨ç‚¹ç§¯å½¢å¼æ¥ç®€æ´åœ°è¡¨è¾¾æ¨¡å‹ï¼š
$$
\hat{y} = \mathbf{w}^\top \mathbf{x} + b.
$$
æˆ‘ä»¬é€šè¿‡å¯¹å™ªå£°åˆ†å¸ƒçš„å‡è®¾æ¥è§£è¯»å¹³æ–¹æŸå¤±ç›®æ ‡å‡½æ•°ã€‚

æ­£æ€åˆ†å¸ƒå’Œçº¿æ€§å›å½’ä¹‹é—´çš„å…³ç³»å¾ˆå¯†åˆ‡ã€‚æ­£æ€åˆ†å¸ƒï¼ˆnormal distributionï¼‰ï¼Œä¹Ÿç§°ä¸º**é«˜æ–¯åˆ†å¸ƒ**ï¼ˆGaussian distributionï¼‰ï¼Œ

ç®€å•çš„è¯´ï¼Œè‹¥éšæœºå˜é‡$x$å…·æœ‰å‡å€¼$\mu$å’Œæ–¹å·®$\sigma^2$ï¼ˆæ ‡å‡†å·®$\sigma$ï¼‰ï¼Œå…¶æ­£æ€åˆ†å¸ƒæ¦‚ç‡å¯†åº¦å‡½æ•°å¦‚ä¸‹ï¼š
$$
p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).
$$
å°±åƒæˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæ”¹å˜å‡å€¼ä¼šäº§ç”Ÿæ²¿$x$è½´çš„åç§»ï¼Œå¢åŠ æ–¹å·®å°†ä¼šåˆ†æ•£åˆ†å¸ƒã€é™ä½å…¶å³°å€¼ã€‚

å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°ï¼ˆç®€ç§°å‡æ–¹æŸå¤±ï¼‰å¯ä»¥ç”¨äºçº¿æ€§å›å½’çš„ä¸€ä¸ªåŸå› æ˜¯ï¼šæˆ‘ä»¬å‡è®¾äº†è§‚æµ‹ä¸­åŒ…å«å™ªå£°ï¼Œå…¶ä¸­å™ªå£°æœä»æ­£æ€åˆ†å¸ƒã€‚å™ªå£°æ­£æ€åˆ†å¸ƒå¦‚ä¸‹å¼:
$$
y = \mathbf{w}^\top \mathbf{x} + b + \epsilon,
$$
å…¶ä¸­ï¼Œ$\epsilon \sim \mathcal{N}(0, \sigma^2)$ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å†™å‡ºé€šè¿‡ç»™å®šçš„$\mathbf{x}$è§‚æµ‹åˆ°ç‰¹å®š$y$çš„**ä¼¼ç„¶**ï¼ˆlikelihoodï¼‰ï¼š
$$
P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).
$$
ç°åœ¨ï¼Œæ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡æ³•ï¼Œå‚æ•°$\mathbf{w}$å’Œ$b$çš„æœ€ä¼˜å€¼æ˜¯ä½¿æ•´ä¸ªæ•°æ®é›†çš„**ä¼¼ç„¶**æœ€å¤§çš„å€¼ï¼š
$$
P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).
$$
æ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡æ³•é€‰æ‹©çš„ä¼°è®¡é‡ç§°ä¸º**æå¤§ä¼¼ç„¶ä¼°è®¡é‡**ã€‚è™½ç„¶ä½¿è®¸å¤šæŒ‡æ•°å‡½æ•°çš„ä¹˜ç§¯æœ€å¤§åŒ–çœ‹èµ·æ¥å¾ˆå›°éš¾ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥åœ¨ä¸æ”¹å˜ç›®æ ‡çš„å‰æä¸‹ï¼Œé€šè¿‡æœ€å¤§åŒ–ä¼¼ç„¶å¯¹æ•°æ¥ç®€åŒ–ã€‚ç”±äºå†å²åŸå› ï¼Œä¼˜åŒ–é€šå¸¸æ˜¯è¯´æœ€å°åŒ–è€Œä¸æ˜¯æœ€å¤§åŒ–ã€‚

æˆ‘ä»¬å¯ä»¥æ”¹ä¸º**æœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶**$-\log P(\mathbf y \mid \mathbf X)$ã€‚ç”±æ­¤å¯ä»¥å¾—åˆ°çš„æ•°å­¦å…¬å¼æ˜¯ï¼š
$$
-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.
$$
ç°åœ¨æˆ‘ä»¬åªéœ€è¦å‡è®¾$\sigma$æ˜¯æŸä¸ªå›ºå®šå¸¸æ•°å°±å¯ä»¥å¿½ç•¥ç¬¬ä¸€é¡¹ï¼Œ

- å› ä¸ºç¬¬ä¸€é¡¹ä¸ä¾èµ–äº$\mathbf{w}$å’Œ$b$ã€‚
- ç°åœ¨ç¬¬äºŒé¡¹é™¤äº†å¸¸æ•°$\frac{1}{\sigma^2}$å¤–ï¼Œå…¶ä½™éƒ¨åˆ†å’Œå‰é¢ä»‹ç»çš„å‡æ–¹è¯¯å·®æ˜¯ä¸€æ ·çš„ã€‚

å¾—åˆ°çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±å‡½æ•°
$$
\min_{\mathbf{w},b} \sum(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b)^2
$$
å¹¸è¿çš„æ˜¯ï¼Œä¸Šé¢å¼å­çš„è§£å¹¶ä¸ä¾èµ–äº$\sigma$ã€‚

å› æ­¤ï¼Œåœ¨é«˜æ–¯å™ªå£°çš„å‡è®¾ä¸‹ï¼Œæœ€å°åŒ–å‡æ–¹è¯¯å·®ç­‰ä»·äºå¯¹çº¿æ€§æ¨¡å‹çš„æå¤§ä¼¼ç„¶ä¼°è®¡ã€‚

## FC layer

å…¨è¿æ¥å±‚æ˜¯â€œå®Œå…¨â€è¿æ¥çš„ï¼Œå¯èƒ½æœ‰å¾ˆå¤šå¯å­¦ä¹ çš„å‚æ•°ã€‚

å…·ä½“æ¥è¯´ï¼Œå¯¹äºä»»ä½•å…·æœ‰$d$ä¸ªè¾“å…¥å’Œ$q$ä¸ªè¾“å‡ºçš„å…¨è¿æ¥å±‚ï¼Œå‚æ•°å¼€é”€ä¸º$\mathcal{O}(dq)$ï¼Œè¿™ä¸ªæ•°å­—åœ¨å®è·µä¸­å¯èƒ½é«˜å¾—ä»¤äººæœ›è€Œå´æ­¥ã€‚å¹¸è¿çš„æ˜¯ï¼Œå°†$d$ä¸ªè¾“å…¥è½¬æ¢ä¸º$q$ä¸ªè¾“å‡ºçš„æˆæœ¬å¯ä»¥å‡å°‘åˆ°$\mathcal{O}(\frac{dq}{n})$ï¼Œå…¶ä¸­è¶…å‚æ•°$n$å¯ä»¥ç”±æˆ‘ä»¬çµæ´»æŒ‡å®šï¼Œä»¥åœ¨å®é™…åº”ç”¨ä¸­å¹³è¡¡å‚æ•°èŠ‚çº¦å’Œæ¨¡å‹æœ‰æ•ˆæ€§

# Softmax-Regression

> classification

softmaxå‡½æ•°èƒ½å¤Ÿå°†æœªè§„èŒƒåŒ–çš„é¢„æµ‹å˜æ¢ä¸ºéè´Ÿæ•°å¹¶ä¸”æ€»å’Œä¸º1ï¼ŒåŒæ—¶è®©æ¨¡å‹ä¿æŒå¯å¯¼çš„æ€§è´¨ã€‚ä¸ºäº†å®Œæˆè¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹æ¯ä¸ªæœªè§„èŒƒåŒ–çš„é¢„æµ‹æ±‚å¹‚ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿è¾“å‡ºéè´Ÿã€‚ä¸ºäº†ç¡®ä¿æœ€ç»ˆè¾“å‡ºçš„æ¦‚ç‡å€¼æ€»å’Œä¸º1ï¼Œæˆ‘ä»¬å†è®©æ¯ä¸ªæ±‚å¹‚åçš„ç»“æœé™¤ä»¥å®ƒä»¬çš„æ€»å’Œã€‚å¦‚ä¸‹å¼ï¼š
$$
\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{å…¶ä¸­}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}
$$
è¿™é‡Œï¼Œå¯¹äºæ‰€æœ‰çš„$j$æ€»æœ‰$0 \leq \hat{y}_j \leq 1$ã€‚å› æ­¤ï¼Œ$\hat{\mathbf{y}}$å¯ä»¥è§†ä¸ºä¸€ä¸ªæ­£ç¡®çš„æ¦‚ç‡åˆ†å¸ƒã€‚

softmaxè¿ç®—ä¸ä¼šæ”¹å˜æœªè§„èŒƒåŒ–çš„é¢„æµ‹$\mathbf{o}$ä¹‹é—´çš„å¤§å°æ¬¡åºï¼Œåªä¼šç¡®å®šåˆ†é…ç»™æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œåœ¨é¢„æµ‹è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥ç”¨ä¸‹å¼æ¥é€‰æ‹©æœ€æœ‰å¯èƒ½çš„ç±»åˆ«ã€‚
$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
$$
å°½ç®¡softmaxæ˜¯ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œä½†softmaxå›å½’çš„è¾“å‡ºä»ç„¶ç”±è¾“å…¥ç‰¹å¾çš„ä»¿å°„å˜æ¢å†³å®šã€‚

å› æ­¤ï¼Œsoftmaxå›å½’æ˜¯ä¸€ä¸ª**çº¿æ€§æ¨¡å‹**ï¼ˆlinear modelï¼‰

è¿™é‡Œæˆ‘ä»¬éœ€è¦äº¤å‰ç†µæŸå¤±å‡½æ•°
$$
 l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j
$$
åˆ©ç”¨softmaxçš„å®šä¹‰ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
$$
\begin{aligned}

l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\

&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\

&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.

\end{aligned}
$$
è€ƒè™‘ç›¸å¯¹äºä»»ä½•æœªè§„èŒƒåŒ–çš„é¢„æµ‹$o_j$çš„å¯¼æ•°ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$
æ¢å¥è¯è¯´ï¼Œå¯¼æ•°æ˜¯æˆ‘ä»¬softmaxæ¨¡å‹åˆ†é…çš„æ¦‚ç‡ä¸å®é™…å‘ç”Ÿçš„æƒ…å†µï¼ˆç”±ç‹¬çƒ­æ ‡ç­¾å‘é‡è¡¨ç¤ºï¼‰ä¹‹é—´çš„å·®å¼‚ã€‚

ä»è¿™ä¸ªæ„ä¹‰ä¸Šè®²ï¼Œè¿™ä¸æˆ‘ä»¬åœ¨å›å½’ä¸­çœ‹åˆ°çš„éå¸¸ç›¸ä¼¼ï¼Œå…¶ä¸­æ¢¯åº¦æ˜¯è§‚æµ‹å€¼$y$å’Œä¼°è®¡å€¼$\hat{y}$ä¹‹é—´çš„å·®å¼‚ã€‚è¿™ä¸æ˜¯å·§åˆï¼Œåœ¨ä»»ä½•æŒ‡æ•°æ—åˆ†å¸ƒæ¨¡å‹ä¸­ï¼ˆå‚è§[link](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html)ï¼‰ï¼Œå¯¹æ•°ä¼¼ç„¶çš„æ¢¯åº¦æ­£æ˜¯ç”±æ­¤å¾—å‡ºçš„ã€‚è¿™ä½¿æ¢¯åº¦è®¡ç®—åœ¨å®è·µä¸­å˜å¾—å®¹æ˜“å¾ˆå¤šã€‚

### å®ç°

æˆ‘ä»¬è®¡ç®—äº†æ¨¡å‹çš„è¾“å‡ºï¼Œç„¶åå°†æ­¤è¾“å‡ºé€å…¥äº¤å‰ç†µæŸå¤±ã€‚

ä»æ•°å­¦ä¸Šè®²ï¼Œè¿™æ˜¯ä¸€ä»¶å®Œå…¨åˆç†çš„äº‹æƒ…ã€‚ç„¶è€Œï¼Œä»è®¡ç®—è§’åº¦æ¥çœ‹ï¼ŒæŒ‡æ•°å¯èƒ½ä¼šé€ æˆæ•°å€¼ç¨³å®šæ€§é—®é¢˜ã€‚å›æƒ³ä¸€ä¸‹ï¼Œsoftmaxå‡½æ•°$\hat y_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$ï¼Œå…¶ä¸­$\hat y_j$æ˜¯é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒã€‚$o_j$æ˜¯æœªè§„èŒƒåŒ–çš„é¢„æµ‹$\mathbf{o}$çš„ç¬¬$j$ä¸ªå…ƒç´ ã€‚å¦‚æœ$o_k$ä¸­çš„ä¸€äº›æ•°å€¼éå¸¸å¤§ï¼Œé‚£ä¹ˆ$\exp(o_k)$å¯èƒ½å¤§äºæ•°æ®ç±»å‹å®¹è®¸çš„æœ€å¤§æ•°å­—ï¼Œå³**ä¸Šæº¢**ï¼ˆoverflowï¼‰ã€‚è¿™å°†ä½¿åˆ†æ¯æˆ–åˆ†å­å˜ä¸º`inf`ï¼ˆæ— ç©·å¤§ï¼‰ï¼Œæœ€åå¾—åˆ°çš„æ˜¯0ã€`inf`æˆ–`nan`ï¼ˆä¸æ˜¯æ•°å­—ï¼‰çš„$\hat y_j$ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ— æ³•å¾—åˆ°ä¸€ä¸ªæ˜ç¡®å®šä¹‰çš„äº¤å‰ç†µå€¼ã€‚

è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ä¸ªæŠ€å·§æ˜¯ï¼šåœ¨ç»§ç»­softmaxè®¡ç®—ä¹‹å‰ï¼Œå…ˆä»æ‰€æœ‰$o_k$ä¸­å‡å»$\max(o_k)$ã€‚è¿™é‡Œå¯ä»¥çœ‹åˆ°æ¯ä¸ª$o_k$æŒ‰å¸¸æ•°è¿›è¡Œçš„ç§»åŠ¨ä¸ä¼šæ”¹å˜softmaxçš„è¿”å›å€¼ï¼š
$$
\begin{aligned}

\hat y_j & =  \frac{\exp(o_j - \max(o_k))\exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k))\exp(\max(o_k))} \\

& = \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.

\end{aligned}
$$
åœ¨å‡æ³•å’Œè§„èŒƒåŒ–æ­¥éª¤ä¹‹åï¼Œå¯èƒ½æœ‰äº›$o_j - \max(o_k)$å…·æœ‰è¾ƒå¤§çš„è´Ÿå€¼ã€‚ç”±äºç²¾åº¦å—é™ï¼Œ$\exp(o_j - \max(o_k))$å°†æœ‰æ¥è¿‘é›¶çš„å€¼ï¼Œå³**ä¸‹æº¢**ï¼ˆunderflowï¼‰ã€‚è¿™äº›å€¼å¯èƒ½ä¼šå››èˆäº”å…¥ä¸ºé›¶ï¼Œä½¿$\hat y_j$ä¸ºé›¶ï¼Œå¹¶ä¸”ä½¿å¾—$\log(\hat y_j)$çš„å€¼ä¸º`-inf`ã€‚åå‘ä¼ æ’­å‡ æ­¥åï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå‘ç°è‡ªå·±é¢å¯¹ä¸€å±å¹•å¯æ€•çš„`nan`ç»“æœã€‚å°½ç®¡æˆ‘ä»¬è¦è®¡ç®—æŒ‡æ•°å‡½æ•°ï¼Œä½†æˆ‘ä»¬æœ€ç»ˆåœ¨è®¡ç®—äº¤å‰ç†µæŸå¤±æ—¶ä¼šå–å®ƒä»¬çš„å¯¹æ•°ã€‚

é€šè¿‡å°†softmaxå’Œäº¤å‰ç†µç»“åˆåœ¨ä¸€èµ·ï¼Œå¯ä»¥é¿å…åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å¯èƒ½ä¼šå›°æ‰°æˆ‘ä»¬çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜ã€‚å¦‚ä¸‹é¢çš„ç­‰å¼æ‰€ç¤ºï¼Œæˆ‘ä»¬é¿å…è®¡ç®—$\exp(o_j - \max(o_k))$ï¼Œè€Œå¯ä»¥ç›´æ¥ä½¿ç”¨$o_j - \max(o_k)$ï¼Œå› ä¸º$\log(\exp(\cdot))$è¢«æŠµæ¶ˆäº†ã€‚
$$
\begin{aligned}

\log{(\hat y_j)} & = \log\left( \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\

& = \log{(\exp(o_j - \max(o_k)))}-\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} \\

& = o_j - \max(o_k) -\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)}.

\end{aligned}
$$
æˆ‘ä»¬ä¹Ÿå¸Œæœ›ä¿ç•™ä¼ ç»Ÿçš„softmaxå‡½æ•°ï¼Œä»¥å¤‡æˆ‘ä»¬éœ€è¦è¯„ä¼°é€šè¿‡æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰å°†softmaxæ¦‚ç‡ä¼ é€’åˆ°æŸå¤±å‡½æ•°ä¸­ï¼Œè€Œæ˜¯[**åœ¨äº¤å‰ç†µæŸå¤±å‡½æ•°ä¸­ä¼ é€’æœªè§„èŒƒåŒ–çš„é¢„æµ‹ï¼Œå¹¶åŒæ—¶è®¡ç®—softmaxåŠå…¶å¯¹æ•°**]ï¼Œè¿™æ˜¯ä¸€ç§ç±»ä¼¼["LogSumExpæŠ€å·§"](https://en.wikipedia.org/wiki/LogSumExp)çš„èªæ˜æ–¹å¼ã€‚

> `LogSumExp`ï¼ˆ**å¯¹æ•°â€“æŒ‡æ•°æ±‚å’ŒæŠ€å·§**ï¼‰æ˜¯æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­éå¸¸å¸¸è§çš„ä¸€ä¸ªæ•°å­¦æŠ€å·§ï¼Œç”¨æ¥ **é¿å…æ•°å€¼æº¢å‡ºã€æé«˜ç¨³å®šæ€§**ã€‚
>
> - æ ¸å¿ƒæ€æƒ³
>
>   åœ¨è®¡ç®—å¦‚ä¸‹è¡¨è¾¾å¼æ—¶ï¼š
>
>   $$
>   \log \left( \sum_i e^{z_i} \right)
>   $$
>   ç›´æ¥è®¡ç®—å¯èƒ½å¯¼è‡´æ•°å€¼æº¢å‡ºæˆ–ä¸‹æº¢ï¼Œå› ä¸ºï¼š
>
>   - è‹¥ \(z_i\) å¾ˆå¤§ï¼Œ\(e^{z_i}\) ä¼šéå¸¸å¤§ â†’ **ä¸Šæº¢ (overflow)**
>   - è‹¥ \(z_i\) å¾ˆå°ï¼Œ\(e^{z_i}\) ä¼šéå¸¸æ¥è¿‘ 0 â†’ **ä¸‹æº¢ (underflow)**
>
>   ---
>
>   ğŸ’¡ è§£å†³æ–¹æ³•ï¼šæå–æœ€å¤§å€¼ \($m = \max_i z_i$\)
>   $$
>   \log \left( \sum_i e^{z_i} \right)
>   = \log \left( e^{m} \sum_i e^{z_i - m} \right)
>   = m + \log \left( \sum_i e^{z_i - m} \right)
>   $$
>   è¿™æ ·ï¼š
>
>   - æ‰€æœ‰çš„ \(z_i - m \le 0\)ï¼Œå› æ­¤ \(e^{z_i - m} \in (0, 1]\)
>   - é¿å…äº†æŒ‡æ•°è¿ç®—çš„æ•°å€¼çˆ†ç‚¸æˆ–æ¶ˆå¤±
>   - æ•°å­¦ä¸Šå®Œå…¨ç­‰ä»·ï¼Œæ•°å€¼ä¸Šæ›´åŠ ç¨³å®š

## å°æ‰¹é‡æ ·æœ¬çŸ¢é‡åŒ–

ä¸ºäº†æé«˜è®¡ç®—æ•ˆç‡å¹¶ä¸”å……åˆ†åˆ©ç”¨GPUï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå¯¹å°æ‰¹é‡æ ·æœ¬çš„æ•°æ®æ‰§è¡ŒçŸ¢é‡è®¡ç®—ã€‚

å‡è®¾æˆ‘ä»¬è¯»å–äº†ä¸€ä¸ªæ‰¹é‡çš„æ ·æœ¬$\mathbf{X}$ï¼Œå…¶ä¸­ç‰¹å¾ç»´åº¦ï¼ˆè¾“å…¥æ•°é‡ï¼‰ä¸º$d$ï¼Œæ‰¹é‡å¤§å°ä¸º$n$ã€‚æ­¤å¤–ï¼Œå‡è®¾æˆ‘ä»¬åœ¨è¾“å‡ºä¸­æœ‰$q$ä¸ªç±»åˆ«ã€‚é‚£ä¹ˆå°æ‰¹é‡æ ·æœ¬çš„ç‰¹å¾ä¸º$\mathbf{X} \in \mathbb{R}^{n \times d}$ï¼Œæƒé‡ä¸º$\mathbf{W} \in \mathbb{R}^{d \times q}$ï¼Œåç½®ä¸º$\mathbf{b} \in \mathbb{R}^{1\times q}$ã€‚

softmaxå›å½’çš„çŸ¢é‡è®¡ç®—è¡¨è¾¾å¼ä¸ºï¼š
$$
\begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned}
$$
ç›¸å¯¹äºä¸€æ¬¡å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œå°æ‰¹é‡æ ·æœ¬çš„çŸ¢é‡åŒ–åŠ å¿«äº†$\mathbf{X}å’Œ\mathbf{W}$çš„çŸ©é˜µ-å‘é‡ä¹˜æ³•ã€‚

ç”±äº$\mathbf{X}$ä¸­çš„æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ•°æ®æ ·æœ¬ï¼Œé‚£ä¹ˆsoftmaxè¿ç®—å¯ä»¥**æŒ‰è¡Œ**ï¼ˆrowwiseï¼‰æ‰§è¡Œï¼š

å¯¹äº$\mathbf{O}$çš„æ¯ä¸€è¡Œï¼Œæˆ‘ä»¬å…ˆå¯¹æ‰€æœ‰é¡¹è¿›è¡Œå¹‚è¿ç®—ï¼Œç„¶åé€šè¿‡æ±‚å’Œå¯¹å®ƒä»¬è¿›è¡Œæ ‡å‡†åŒ–ã€‚

## Loss Function

- å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±å‡½æ•°
  $$
  \min_{\mathbf{w},b} \sum(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b)^2
  $$

- äº¤å‰ç†µæŸå¤±å‡½æ•°
  $$
   l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j
  $$

# å®ç°

1. å¼•å…¥åº“

   ```python
   import numpy as np
   import torch
   from torch.utils import data
   from d2l import torch as d2l
   ```

2. è¯»æ•°æ®

   ```python
   def load_data_fashion_mnist(batch_size, resize=None):  #@save
       """ä¸‹è½½Fashion-MNISTæ•°æ®é›†ï¼Œç„¶åå°†å…¶åŠ è½½åˆ°å†…å­˜ä¸­"""
       trans = [transforms.ToTensor()]
       if resize:
           trans.insert(0, transforms.Resize(resize))
       trans = transforms.Compose(trans)
       mnist_train = torchvision.datasets.FashionMNIST(
           root="../data", train=True, transform=trans, download=True)
       mnist_test = torchvision.datasets.FashionMNIST(
           root="../data", train=False, transform=trans, download=True)
       return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                               num_workers=get_dataloader_workers()),
               data.DataLoader(mnist_test, batch_size, shuffle=False,
                               num_workers=get_dataloader_workers()))

       train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
   ```

3. å®šä¹‰æ¨¡å‹

   ```
   # nnæ˜¯ç¥ç»ç½‘ç»œçš„ç¼©å†™
   from torch import nn

   net = nn.Sequential(nn.Linear(2, 1))
   ```

   åˆå§‹è¯æ¨¡å‹å‚æ•°ï¼ˆå¯èƒ½ä¸éœ€è¦ï¼‰

   ```
   net[0].weight.data.normal_(0, 0.01)
   net[0].bias.data.fill_(0)
   ```

4. å®šä¹‰æŸå¤±å‡½æ•°

   ```
   loss = nn.MSELoss()
   ```

5. å®šä¹‰ä¼˜åŒ–ç®—æ³•

   ```
   trainer = torch.optim.SGD(net.parameters(), lr=0.03)
   ```

6. è®­ç»ƒ

   ```
   num_epochs = 3
   for epoch in range(num_epochs):
       for X, y in data_iter:
           l = loss(net(X) ,y)
           trainer.zero_grad()
           l.backward()
           trainer.step()
       l = loss(net(features), labels)
       print(f'epoch {epoch + 1}, loss {l:f}')
   ```
