# Main Takeaway

神经网络与梯度下降

- [ ] 掌握神经网络优化算法：SGD、Adam等
- [ ] 理解深度学习的控制论视角

<!--more-->



# 神经网络的基本原理

凸优化问题有助于分析算法的特点。 毕竟对大多数非凸问题来说，获得有意义的理论保证很难，但是直觉和洞察往往会延续。所以我们研究一个优化算法，常常将其运用于如下问题：
$$
\min f(x)=\frac{1}{2}x^TQx+c^Tx+b
$$

对于损失函数，两种方法：

- 梯度下降
- 反向传播

## 大规模优化挑战

在统计学和机器学习中，许多问题的核心可归结为以下优化问题：
$$
\min_x f(x) = \frac{1}{m} \sum_{i=1}^m f_i(x)
$$
其中$m$表示样本量（即子函数个数）

- 计算挑战：当$m$极大时，全梯度计算需要巨大计算量，甚至在$m$为无穷时无法实现。


- 解决策略：用随机子函数的梯度来估计目标函数的全梯度，这类方法称为随机方法。在实际应用中，这类方法通常比确定性的梯度下降方法更快



# 梯度下降方法的变体



## 随机梯度下降SGD

SGD（stochastic gradient descent）

- 核心思想：每次迭代中随机选择一个样本i，然后利用该样本的梯度来更新参数

  为什么SGD能有效果？我们强调随机梯度$\nabla f_i(x)$是对完整梯度$\nabla f(x)$的无偏估计：
  $$
  E_i\nabla f_i(x)=\frac{1}{n}\sum \nabla f_i(x) = \nabla f(x)
  $$

- 算法步骤
  
  1. 在每次迭代中随机选择一个样本 $i \sim \text{Uniform}(1,m)$
  
  2. 计算单个样本的梯度$\nabla f_i(x_t)$
  
  3. 参数更新$x_{t+1} = x_t - \eta \nabla f_i(x_t)$，其中 $\eta$ 为学习率 (learning rate)

- 特性分析

  | 优点             | 缺点               |
  | ---------------- | ------------------ |
  | 单次迭代计算量低 | 收敛路径震荡       |
  | 能逃离局部极小点 | 学习率需精细调节   |
  | 适合大规模数据集 | 不保证严格单调收敛 |

## 小批量梯度下降Mini-batch Gradient Descent

- 核心思想：每次迭代中随机选择一个小批量样本，然后利用这些样本的梯度平均值来更新参数——**计算效率**

- 特性分析：小批量梯度下降在SGD和批量梯度下降之间取得平衡，既减少了计算量，又降低了更新的波动性。

  好的副作用：使用平均梯度减小了方差

## 动量算法：

- 核心思想：在迭代过程中引入动量项，利用历史梯度信息来加速收敛并减少震荡
  $$
  v_t = \beta v_{t-1}+g_{t,t-1},~\beta \in(0,1)
  $$
  

## Nesterov加速梯度方法（Nesterov Accelerated Gradient, NAG）

- 核心思想：改进的动量优化算法，其核心在于**前瞻性梯度计算**。与传统动量法（Momentum）不同，NAG 在更新参数时先沿当前动量方向进行一步预测，然后在预测点计算梯度，从而更准确地调整参数方向

- 算法步骤：

  1. 生成辅助变量$v_k$：$v_k=x_k+\gamma(x_k-x_{k-1})$

     其中，$\gamma$是动量参数，常取0.9

  2. 梯度下降：
     $$
     x_{k+1}=v_k-\alpha_k \nabla f(v_k)
     $$
     其中，$\alpha_k$是步长

- 优缺点：

  | **优点**                                                     | **缺点**                                               |
  | ------------------------------------------------------------ | ------------------------------------------------------ |
  | **更快的收敛速度**：通过前瞻性梯度修正方向，减少震荡，加速收敛至局部最优（理论收敛速率 O(1/t2)，优于标准梯度下降的 O(1/t)） | **超参数敏感**：需仔细调节学习率 η 和动量系数 γ        |
  | **高曲率适应性强**：在病态条件或高曲率区域表现优于传统动量法 | **非凸问题局限**：对非凸目标函数可能陷入鞍点或局部极小 |
  | **减少震荡**：前瞻梯度计算使更新方向更贴近实际下降路径       | **计算复杂度略高**：需额外计算预测点梯度               |

  特别在处理大规模数据集和高维参数空间时表现出色。

## Adagrad（Adaptive Gradient）

Adagrad是一种随机梯度下降的方法[11.7. AdaGrad算法](https://zh-v2.d2l.ai/chapter_optimization/adagrad.html)

- 引言：稀疏特征，鉴于学习率下降，我们可能最终会面临这样的情况：常见特征的参数相当迅速地收敛到最佳值，而对于不常见的特征，我们仍缺乏足够的观测以确定其最佳值。 换句话说，学习率要么对于常见特征而言降低太慢，要么对于不常见特征而言降低太快。
  
  解决此问题的一个方法就是记录我们看到特定特征的次数，然后将其用作调整学习率：$\eta_i = \frac{\eta_0}{\sqrt{s(i,t)+c}},~s(i,t)$这里计下了我们截至$t$观察到功能$i$的次数

- 核心思想：为每个参数动态调整学习率，是一种**自适应学习率优化算法**
  
  通过将上面粗略的计数器$s(i,t)$替换为先前观察所得梯度的平方之和来解决这个问题
  $$
  s(i,t+1)=s(i,t)+(\partial_if(x))^2
  $$
  
  - **高频参数**（梯度大且频繁更新的参数）降低学习率，避免震荡；
  - **低频参数**（梯度小或稀疏的参数）增大学习率，加速收敛。 通过累积历史梯度平方和，Adagrad 自动适应不同参数的特征，特别适合稀疏数据（如自然语言处理任务）
  
  AdaGrad算法会在单个坐标层面动态降低学习率
  
- 算法步骤：

  - 初始化：

    - 全局学习率$\eta$
    - 梯度平方累计变量$r=0$
    - 小常数$\epsilon$

  - 迭代更新

    - 计算当前梯度

    - 累积梯度平方和：$r_t=r_{t-1}+g_t\odot g_t$

      > $\odot$哈达玛积：两个同维矩阵的元素对应相乘

    - 调整学习率；$\eta_t = \frac{\eta}{\sqrt{r_t+\epsilon}}$

    - 更新参数

- 优缺点

  | **优点**                                                     | **缺点**                                                     |
  | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | **自适应学习率**：无需手动调节，适合高维稀疏数据（如 NLP 中的词向量）。 | **学习率衰减过快**：累积梯度平方和持续增大，导致后期学习率趋近于零，模型停止更新。 |
  | **梯度方向优化**：对低频参数更敏感，加速稀疏特征的学习。     | **内存消耗高**：需为每个参数存储梯度平方累积量，参数量大时内存占用显著。 |
  | **数值稳定性**：引入 ϵ 避免分母为零，适合非凸优化问题。      | **依赖初始学习率**：初始 η 设置不当可能导致早期收敛慢或震荡。 |

## RMSprop（Root Mean Square Propagation）

**Adagrad 算法的改进版本**，旨在解决 Adagrad 因累积全部历史梯度导致学习率过早衰减的问题，Adagrad:$s(i,t+1)=s(i,t)+(\partial_if(x))^2$由于缺乏规范化，没有约束力，$s_t$持续增长，几乎上是在算法收敛时呈线性递增。

- 核心思想：$s(i,t+1)=\gamma s(i,t)+(1-\gamma)(\partial_if(x))^2$
  - **指数加权平均**：通过衰减系数（如 0.9）动态调整历史梯度的影响，仅保留近期梯度信息，避免长期累积导致学习率过小。
  - **自适应学习率**：每个参数的学习率根据其梯度幅度的均方根（RMS）动态调整，梯度大的参数降低学习率，梯度小的参数增大学习率

## Adadelta

Adadelta是AdaGrad的另一种变体

- 核心思想：减少了学习率适应坐标的数量。 此外，广义上Adadelta被称为没有学习率，因为它使用变化量本身作为未来变化的校准

- 算法步骤：

  1. 泄漏梯度平方更新

  维护历史梯度平方的指数加权平均（ρ为衰减因子）：
  $$s_t = \rho s_{t-1} + (1 - \rho) g_t'^2 \tag{11.9.1}$$

  2. 参数更新规则

  使用调整后的梯度进行参数更新：
  $$x_t = x_{t-1} - g_t' \tag{11.9.2}$$

  3. 梯度调整计算

  通过历史参数更新量缩放原始梯度（ϵ为数值稳定项）：
  $$g_t' = \frac{\sqrt{\Delta x_{t-1} + \epsilon}}{\sqrt{s_t + \epsilon}} \odot g_t \tag{11.9.3}$$

  4. 参数更新量更新

  维护参数更新量的指数加权平均：
  $$\Delta x_t = \rho \Delta x_{t-1} + (1 - \rho) g_t'^2 \tag{11.9.4}$$

## Adam（Adaptive Moment Estimation）

一种自适应学习率优化算法，**融合了动量法（Momentum）和RMSProp的优点**。将很多优化技术汇总到了这一个方法里面，但是也有一些问题yogi改进

- 核心思想：

  - **一阶矩估计（动量）**：跟踪梯度的指数加权平均，保持参数更新方向的稳定性。
  - **二阶矩估计（自适应学习率）**：计算梯度平方的指数加权平均，根据梯度幅值调整学习率。
  - **偏差校正**：修正初始阶段因零初始化导致的矩估计偏差，确保更新量的准确性。

- 算法步骤：

  - l初始化：

    - 初始参数：$ \theta_0 $
    - 学习率：$ \alpha = 0.001 $（默认）
    - 一阶矩衰减率：$ \beta_1 = 0.9 $
    - 二阶矩衰减率：$ \beta_2 = 0.999 $
    - 数值稳定常数：$ \epsilon = 10^{-8} $
    - 初始一阶矩 $ m_0 = 0 $，二阶矩 $ v_0 = 0 $

  - 迭代过程（时间步 $ t=1,2,\dots $）：

    1. 计算当前梯度：

    $$
       g_t = \nabla_\theta L(\theta_{t-1})
    $$
    
    2. 更新一阶矩（动量）：
  
    $$
      m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t
    $$
  
    3. 更新二阶矩（自适应学习率）：

    $$
      v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2
    $$
  
    4. 偏差校正（消除初始零偏置）：
  
    $$
      \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
    $$
  
    5. 更新参数：
  
    $$
      \theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \cdot \hat{m}_t
    $$

- 优缺点

  | **优势**         | **说明**                                                     |
  | ---------------- | ------------------------------------------------------------ |
  | **自适应学习率** | 每个参数独立调整学习率，梯度大的方向步长减小，梯度小的方向步长增大。 |
  | **高效收敛**     | 结合动量与自适应机制，在非凸优化问题中收敛速度通常快于SGD、RMSProp。 |
  | **处理稀疏梯度** | 对低频参数（如NLP中的词向量）自动增大更新步长，提升训练效率。 |
  | **抗噪声能力强** | 通过指数加权平均平滑梯度噪声，适应高噪声数据场景。           |

  | **局限性**           | **说明**                                                     |
  | -------------------- | ------------------------------------------------------------ |
  | **超参数敏感**       | β1,β2,α 需调优，不当设置可能导致收敛不稳定。                 |
  | **内存占用较高**     | 需存储一阶和二阶矩变量，参数量较大时内存消耗显著。           |
  | **局部最优风险**     | 在部分非凸问题中可能陷入鞍点，需结合预热（Warmup）或学习率衰减。 |
  | **长期训练性能下降** | 因自适应学习率随训练衰减，后期可能不如带动量的SGD            |



# 控制论视角下的神经网络

Nature Communication上的PIDAO论文：

gradient-based optimizations can be interpreted as continuous-time dynamical systems  将优化过程建模为**连续时间动力系统**，并引入**PID控制器**设计新的优化器，通过反馈控制机制改进优化动态

## HW

作业：机器学习中的优化算法

- 学习神经网络的基本原理，主要从无约束优化的角度，了解反向传播的求解方法，主要参考资料：[An Introduction to Optimization: With Applications to Machine Learning - Edwin K. P. Chong, Wu-Sheng Lu, Stanislaw H. Zak - Google Books](https://books.google.co.jp/books?hl=en&lr=&id=uEDUEAAAQBAJ&oi=fnd&pg=PR15&dq=info:WSaWZthIywYJ:scholar.google.com&ots=qAtUVnxFtM&sig=Sl77RBaLcYWOmv7FMLzQZg-qcGg&redir_esc=y#v=onepage&q&f=false)——这本书的ch13 Unconstrained Optimization and Neural Networks  
- 学习机器学习中重要的梯度类算法
  - SGD、Nestonov等
  - Adagrad优化算法
  - RMSprop和Adam优化算法
- 针对以上的学习内容：撰写一篇小论文，阐述机器学习中的优化问题与加速算法
- 附加研究兴趣题目：深度学习优化中的梯度流、信息几何、动力系统等方法，参考Nature Communication上的PIDAO论文




# References

- https://zhuanlan.zhihu.com/p/629131647
- [Tutorial — Ceres Solver (ceres-solver.org)](http://ceres-solver.org/tutorial.html)

- MlT,theprincipleofoptimal control,thefirsttwolectures
- Dive into deep learning, prepare for the later advanced optimizations in deeplearning
- Lecturesforthe classtoday-Ryan的课程讲义，pdf文件另发