# Main Takeaway

配套CMU-16-745 Optimal Control and Reinforcement Learning食用



<!--more-->



# ZJU-Optimization & Optimal Control

上两节同学讲一节

## Lec 1 intro

这一节主要是介绍了控制论角度和E-L方程

- 深度学习的控制论视角——PID加速优化训练框架（PIDAO）

  深度神经网络-DNN，本质上是，面向信息加工的过程系统设计与控制

  深度神经网络反馈机制缺失

  本质上是一个高纬度的参数优化问题

  多用连续求解（方法多）再离散化

- 瓦特改进[蒸汽机离心调速器原理与应用](https://blog.csdn.net/u013414501/article/details/82428094)——相当于加了反馈

nature-inspired computing



主要介绍两个部分

- 优化optimization——finite 

  - unconstrained opt——PID
  - constrained opt
  - linear programming
  - nonlinear programming——NLP

- 最优控制optimal control——infinite dimensioned optimization

  - 变分法

  - 分析力学建模

  - PMP[module3.pdf (nd.edu)](https://www3.nd.edu/~lemmon/courses/ee565/lectures/module3.pdf)

    [https://www.cimat.mx/~murrieta/HJBandPMP.pdf](https://www.cimat.mx/~murrieta/HJBandPMP.pdf#:~:text=PMP expresses conditions along the optimal trajectory%2C as,optimal control is function of (t) %3D rV(x(t)).)

  - $DP\to RL$



根据[变分法简介Part 1.（Calculus of Variations)](https://zhuanlan.zhihu.com/p/20718489)，得到传说中的欧拉-拉格朗日方程（E-L equation）
$$
\frac{\partial L}{\partial y}-\frac{\partial}{\partial x}\frac{\partial L}{\partial y\prime} = 0
$$
当$L$的表达式中不显含$x$时有$L-y\prime \frac{\partial L}{\partial y\prime}=C$

> 为什么当$L$的表达式中不显含$x$时有这样的形式：
>
> **1. 数学推导：守恒量的形式**
>
> 假设拉格朗日函数 $  L = L(y, y')  $ 不显含自变量 $  x  $，即满足：
>
> $$
> \frac{\partial L}{\partial x} = 0
> $$
> 对 $  L  $ 关于 $  x  $ 求全导数：
>
> $$
> \frac{\mathrm{d}L}{\mathrm{d}x} = \frac{\partial L}{\partial y} \cdot \frac{\mathrm{d}y}{\mathrm{d}x} + \frac{\partial L}{\partial y'} \cdot \frac{\mathrm{d}y'}{\mathrm{d}x} = \frac{\partial L}{\partial y} \cdot y' + \frac{\partial L}{\partial y'} \cdot y''
> $$
> 根据欧拉-拉格朗日方程：
>
> $$
> \frac{\partial L}{\partial y} = \frac{\mathrm{d}}{\mathrm{d}x} \left( \frac{\partial L}{\partial y'} \right)
> $$
>
> 将其代入全导数表达式：
>
> $$
> \frac{\mathrm{d}L}{\mathrm{d}x} = \left[ \frac{\mathrm{d}}{\mathrm{d}x} \left( \frac{\partial L}{\partial y'} \right) \right] \cdot y' + \frac{\partial L}{\partial y'} \cdot y''
> $$
> 观察右侧表达式，可改写为：
>
> $$
> \frac{\mathrm{d}}{\mathrm{d}x} \left( y' \cdot \frac{\partial L}{\partial y'} \right) = y'' \cdot \frac{\partial L}{\partial y'} + y' \cdot \frac{\mathrm{d}}{\mathrm{d}x} \left( \frac{\partial L}{\partial y'} \right)
> $$
>
> 因此有：
>
> $$
> \frac{\mathrm{d}L}{\mathrm{d}x} = \frac{\mathrm{d}}{\mathrm{d}x} \left( y' \cdot \frac{\partial L}{\partial y'} \right)
> $$
> 移项后得到守恒方程：
>
> $$
> \frac{\mathrm{d}}{\mathrm{d}x} \left( L - y' \cdot \frac{\partial L}{\partial y'} \right) = 0
> $$
>
> 积分后得到守恒量：
>
> $$
> L - y' \cdot \frac{\partial L}{\partial y'} = C \quad (\text{常数})
> $$
> ---
>
> **2. 关键公式总结**
>
> | 步骤       | 公式                                                         |
> | ---------- | ------------------------------------------------------------ |
> | 全导数     | $ \frac{\mathrm{d}L}{\mathrm{d}x} = \frac{\partial L}{\partial y} y' + \frac{\partial L}{\partial y'} y'' $ |
> | 守恒量形式 | $ L - y' \cdot \frac{\partial L}{\partial y'} = C $          |

- 推广：[变分法笔记(2)——Euler-Lagrange方程的基础推广](https://zhuanlan.zhihu.com/p/358115697)

  > - Lagrange函数推广到关于y的高阶导数、y是一元向量值函数的情形
  > - 经典力学的数学基础
  > - 推广到y是多元函数的情形

两个使用E-L方程的例子：

- 最速下降线[什么是最速降线？它又有何奇妙的性质呢？](https://zhuanlan.zhihu.com/p/68140784)

  建模后：$L(x,y,y\prime)=\sqrt{\frac{1+(\frac{dy}{dx})^2}{2gy}}$

  ![v2-0f20826aa3a5fcbd4bcaea8f843ce764_b](markdown-img/最优化与最优控制.assets/v2-0f20826aa3a5fcbd4bcaea8f843ce764_b.webp)

- 平面两点直线距离最短[(21 封私信 / 80 条消息) 如何只通过计算证明“两点之间，线段最短”?](https://www.zhihu.com/question/355602892)

## Lec 2 Unconstrained Optimization

这节主要介绍一些常见的优化算法

- 代码可以直接运行：[11. 优化算法 — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/chapter_optimization/index.html)

- MIT优化课件（前两节）[Lecture Notes | Principles of Optimal Control | Aeronautics and Astronautics | MIT OpenCourseWare](https://ocw.mit.edu/courses/16-323-principles-of-optimal-control-spring-2008/pages/lecture-notes/)

  > 课件见courseware



### Mathematical Fundamentals

- 矩阵条件数的看法：[矩阵的条件数](https://zhuanlan.zhihu.com/p/91393594)

  条件数同时描述了矩阵 A 对向量的拉伸能力和压缩能力，换句话说，令向量发生形变的能力。条件数越大，向量在变换后越可能变化得越多。

  减小病态矩阵的影响——加正则项



Gradient, Hessian, Jacobian

The gradient of a scalar function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is defined by
$$
\nabla f(\mathbf{x}) = 
\begin{bmatrix}
\frac{\partial f(\mathbf{x})}{\partial x_1} \\
\frac{\partial f(\mathbf{x})}{\partial x_2} \\
\vdots \\
\frac{\partial f(\mathbf{x})}{\partial x_n}
\end{bmatrix}
= 
\left[ \frac{\partial f(\mathbf{x})}{\partial x_1}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n} \right]^\top
= \left[ \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}} \right]^\top
$$
The Hessian of a scalar function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is defined by
$$
\nabla^2 f(\mathbf{x}) = 
\begin{bmatrix}
\frac{\partial^2 f(\mathbf{x})}{\partial x_1^2} & \frac{\partial^2 f(\mathbf{x})}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f(\mathbf{x})}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f(\mathbf{x})}{\partial x_2 \partial x_1} & \frac{\partial^2 f(\mathbf{x})}{\partial x_2^2} & \cdots & \frac{\partial^2 f(\mathbf{x})}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f(\mathbf{x})}{\partial x_n \partial x_1} & \frac{\partial^2 f(\mathbf{x})}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f(\mathbf{x})}{\partial x_n^2}
\end{bmatrix}
= \frac{\partial}{\partial \mathbf{x}} [\nabla f(\mathbf{x})]
$$
The Jacobian of a vector-valued function $f : \mathbb{R}^n \rightarrow \mathbb{R}^m$ is defined by
$$
\frac{\partial \mathbf{f}(\mathbf{x})}{\partial \mathbf{x}} = 
\begin{bmatrix}
\frac{\partial f_1(\mathbf{x})}{\partial x_1} & \frac{\partial f_1(\mathbf{x})}{\partial x_2} & \cdots & \frac{\partial f_1(\mathbf{x})}{\partial x_n} \\
\frac{\partial f_2(\mathbf{x})}{\partial x_1} & \frac{\partial f_2(\mathbf{x})}{\partial x_2} & \cdots & \frac{\partial f_2(\mathbf{x})}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_m(\mathbf{x})}{\partial x_1} & \frac{\partial f_m(\mathbf{x})}{\partial x_2} & \cdots & \frac{\partial f_m(\mathbf{x})}{\partial x_n}
\end{bmatrix}
$$
Mean Value Theorem for Vector-Valued Functions

Let $\mathbf{f} : \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a vector-valued function. The **mean value theorem** states:
$$
\mathbf{f}(\mathbf{x} + \mathbf{p}) - \mathbf{f}(\mathbf{x}) = \int_0^1 \frac{\partial \mathbf{f}(\mathbf{x} + \eta \mathbf{p})}{\partial \mathbf{x}} \mathbf{p} \, d\eta
$$
where:
- $\frac{\partial \mathbf{f}(\mathbf{x})}{\partial \mathbf{x}}$ is the Jacobian matrix of $\mathbf{f}$ at $\mathbf{x}$
- $\eta \in [0, 1]$ parametrizes the integration path between $\mathbf{x}$ and $\mathbf{x}+\mathbf{p}$
- The integral represents an averaged linear approximation of $\mathbf{f}$ over the line segment connecting $\mathbf{x}$ and $\mathbf{x}+\mathbf{p}$



若函数 $f(\mathbf{x})$ 对其所有变量的二阶偏导数连续，则有：
$$
\frac{\partial^2 f(\mathbf{x})}{\partial x_i \partial x_j} = \frac{\partial^2 f(\mathbf{x})}{\partial x_j \partial x_i}

$$
此时函数的Hessian矩阵为对称矩阵，矩阵满足 $\nabla^2 f = (\nabla^2 f)^\top$。

应用意义

- **优化理论**：Hessian矩阵的对称性是判断极值点（凸性/凹性）的基础。
- **泰勒展开**：二阶泰勒展开式 $f(\mathbf{x}+\mathbf{h}) \approx f(\mathbf{x}) + \nabla f \cdot \mathbf{h} + \frac{1}{2} \mathbf{h}^\top \nabla^2 f \mathbf{h}$ 依赖此对称性。
- **物理学方程**：连续介质力学、电磁学中的场方程需满足此条件以保证解的存在性。



### optimation

**下面介绍一些优化算法：**

- 梯度下降：gradient flow梯度流的介绍：[梯度流：探索通向最小值之路](https://kexue.fm/archives/9660)

  梯度流是将我们在用梯度下降法中寻找最小值的过程中的各个点连接起来，形成一条随（虚拟的）时间变化的轨迹，这条轨迹便被称作“梯度流”。

  最速方向：为什么“梯度的负方向是局部下降最快的方向”，实际上是在在欧氏空间中且有约束条件$||x-x_t||=\epsilon$
  $$
  x_{t+1}=x_t -\gamma \nabla f(x_t)
  $$
  所以这是一个带约束的优化，常将其转化为：
  $$
  x_{t+1}=\arg \min_x \frac{||x-x_t||^2}{2\alpha}+f(x)
  $$
  将约束当成惩罚项加入到优化目标，这样就不用考虑求解约束，也容易推广

  > 根据不同的正则项，可以行程不同的梯度下降方案

  搜索方向$d_t=-\nabla f(x_t)$，若采用精确线性搜索即$\gamma_k = \arg \min_{\gamma>0}f(x_{t}+\gamma d_t)$

  则有：
  $$
  \frac{df(x_t+\gamma d_t)}{d\gamma}|_{\gamma=\gamma_t} =(d_t)^T\nabla f(x_{t+1})=0
  $$
  这表明相邻两次的搜索方向$d_t$ and $d_{t+1}$是正交的

  <img src="markdown-img/最优化与最优控制.assets/image-20250311140727165.png" alt="image-20250311140727165" style="zoom:50%;" />

- Newton`s Method
  
  在当前迭代点对目标函数$f(x)$进行二阶泰勒展开，构造一个近似二次函数，并求解这个二次函数的极值点作为下一步的迭代点
  $$
  x_{t+1} =x_t +step = x_t - \nabla^2 f(x_t)^{-1} \nabla f(x_t)
  $$
  
- 先介绍**Sherman-Morrison-Woodbury Formula**

  是矩阵逆计算的重要工具，适用于对可逆矩阵进行低秩修正的场景。

  **核心思想：通过低秩修正项快速更新逆矩阵，避免直接计算大规模矩阵的逆**
  $$
  (A + U V^\top)^{-1} = A^{-1} - A^{-1} U (I_k + V^\top A^{-1} U)^{-1} V^\top A^{-1}
  $$
  当秩为1时：
  $$
  (A + U V^\top)^{-1} = A^{-1} - \frac{A^{-1} U  V^\top A^{-1}}{1 + V^\top A^{-1} U}
  $$

- Quasi-Newton Method拟牛顿法[拟牛顿法与SR1,DFP,BFGS三种拟牛顿算法](https://zhuanlan.zhihu.com/p/306635632)

  Hessian矩阵求逆太难了，找了个$H_K$来替代

  通过在牛顿法的迭代中加入近似求取每一步Hessian矩阵的迭代步，仅通过迭代点处的梯度信息来求取Hessian矩阵的近似值

  - DFP Method（Davidon-Fletcher-Powell）

    **目标**：

    **拟牛顿条件**：在迭代优化中，希望近似Hessian逆矩阵 $  H_{k+1}  $ 满足：
    $$
    H_{k+1} \Delta g_k = \Delta x_k \quad \text{其中} \quad 
    \begin{cases} 
    \Delta x_k = x_{k+1} - x_k \\
    \Delta g_k = \nabla f(x_{k+1}) - \nabla f(x_k)
    \end{cases}
    $$
    **DFP更新公式**：
    $$
    H_{k+1} = H_k + \frac{\Delta x_k \Delta x_k^\top}{\Delta x_k^\top \Delta g_k} - \frac{H_k \Delta g_k \Delta g_k^\top H_k}{\Delta g_k^\top H_k \Delta g_k}
    $$
    **正向推导**：

    假设Hessian逆的更新形式为秩2修正：

    $$
    H_{k+1} = H_k + \beta u u^\top + \gamma v v^\top
    $$

    其中 $  u, v \in \mathbb{R}^n  $，$  \beta, \gamma \in \mathbb{R}  $。

    将修正形式代入 $  H_{k+1} \Delta g_k = \Delta x_k  $：

    $$
    H_k \Delta g_k + \beta u (u^\top \Delta g_k) + \gamma v (v^\top \Delta g_k) = \Delta x_k
    $$
    为了简化方程，选择基向量与物理量直接相关：

    - **方向1**：参数变化方向 $  u = \Delta x_k  $
    - **方向2**：梯度变化方向 $  v = H_k \Delta g_k  $

    代入后得到：

    $$
    \beta (\Delta x_k^\top \Delta g_k) \Delta x_k + \gamma (\Delta g_k^\top H_k \Delta g_k) H_k \Delta g_k = \Delta x_k - H_k \Delta g_k
    $$
    令两边系数相等：

    $$
    \beta (\Delta x_k^\top \Delta g_k) = 1 \quad \Rightarrow \quad \beta = \frac{1}{\Delta x_k^\top \Delta g_k}
    $$

    $$
    \gamma (\Delta g_k^\top H_k \Delta g_k) = -1 \quad \Rightarrow \quad \gamma = -\frac{1}{\Delta g_k^\top H_k \Delta g_k}
    $$

    将 $  \beta, \gamma, u, v  $ 代入秩2修正假设：

    $$
    H_{k+1} = H_k + \frac{\Delta x_k \Delta x_k^\top}{\Delta x_k^\top \Delta g_k} - \frac{H_k \Delta g_k \Delta g_k^\top H_k}{\Delta g_k^\top H_k \Delta g_k}
    $$

    ---

    **逆向验证（使用SMW）**：

    目标是通过SMW公式推导其逆矩阵 $  B_{k+1} = H_{k+1}^{-1}  $。

    将DFP公式分解为两个秩1修正项：
    $$
    H_{k+1} = H_k + \underbrace{\frac{\Delta x_k \Delta x_k^\top}{\Delta x_k^\top \Delta g_k}}_{\text{秩1项}} - \underbrace{\frac{H_k \Delta g_k \Delta g_k^\top H_k}{\Delta g_k^\top H_k \Delta g_k}}_{\text{秩1项}}
     
    对每个秩1修正项分别应用Sherman-Morrison公式（SMW的秩1特例）。
    $$
    **第一项修正**（正项）：
    $$
         H^{(1)} = H_k + \frac{\Delta x_k \Delta x_k^\top}{\Delta x_k^\top \Delta g_k}
       
    $$

     设 $  u = \Delta x_k  $, $  v = \frac{\Delta x_k}{\Delta x_k^\top \Delta g_k}  $，则：
    $$
         H^{(1)} = H_k + u v^\top
       
    $$

     应用SMW逆公式：
    $$
    \left( H^{(1)} \right)^{-1} = H_k^{-1} - \frac{H_k^{-1} \Delta x_k \Delta x_k^\top H_k^{-1}}{\Delta x_k^\top \Delta g_k + \Delta x_k^\top H_k^{-1} \Delta x_k}
    $$
    

    **第二项修正**（负项）：
    $$
         H_{k+1} = H^{(1)} - \frac{H_k \Delta g_k \Delta g_k^\top H_k}{\Delta g_k^\top H_k \Delta g_k}
       
    $$

     设 $  u = -H_k \Delta g_k  $, $  v = \frac{H_k \Delta g_k}{\Delta g_k^\top H_k \Delta g_k}  $，则：
    $$
         H_{k+1} = H^{(1)} + u v^\top
       
    $$
    再次应用SMW公式得到最终DFP的$B_{k+1}$更新公式
    $$
    B_{k+1} = \left( I - \frac{\Delta x_k \Delta g_k^\top}{\Delta g_k^\top \Delta x_k} \right) B_k \left( I - \frac{\Delta g_k \Delta x_k^\top}{\Delta g_k^\top \Delta x_k} \right) + \frac{\Delta x_k \Delta x_k^\top}{\Delta g_k^\top \Delta x_k}
    $$
    下面验证其满足 $  H_{k+1} \Delta g_k = \Delta x_k  $：
    将 $  H_{k+1}  $ 代入拟牛顿条件 $  H_{k+1} \Delta g_k = \Delta x_k  $：
    $$
       \begin{aligned}
       H_{k+1} \Delta g_k &= \left( H_k + \frac{\Delta x_k \Delta x_k^\top}{\Delta x_k^\top \Delta g_k} - \frac{H_k \Delta g_k \Delta g_k^\top H_k}{\Delta g_k^\top H_k \Delta g_k} \right) \Delta g_k \\
       &= H_k \Delta g_k + \Delta x_k - H_k \Delta g_k \\
       &= \Delta x_k
       \end{aligned}
     
    $$

       **结论**：公式满足拟牛顿条件。

  - BFGS Method

    **目标：**

    **拟牛顿条件**：在迭代优化中，希望近似Hessian矩阵的逆 $  H_{k+1}  $ 满足：
    $$
    H_{k+1} \Delta g_k = \Delta x_k \quad \text{其中} \quad 
    \begin{cases} 
    \Delta x_k = x_{k+1} - x_k \\
    \Delta g_k = \nabla f(x_{k+1}) - \nabla f(x_k)
    \end{cases}
    $$
    **BFGS更新公式**（Hessian逆矩阵形式）：

    $$
    H_{k+1} = \left( I - \frac{\Delta x_k \Delta g_k^\top}{\Delta g_k^\top \Delta x_k} \right) H_k \left( I - \frac{\Delta g_k \Delta x_k^\top}{\Delta g_k^\top \Delta x_k} \right) + \frac{\Delta x_k \Delta x_k^\top}{\Delta g_k^\top \Delta x_k}
    $$

    ## 

    BFGS直接更新Hessian矩阵 $  B_k  $，其更新规则为：

    $$
    B_{k+1} = B_k + \frac{\Delta g_k \Delta g_k^\top}{\Delta g_k^\top \Delta x_k} - \frac{B_k \Delta x_k \Delta x_k^\top B_k}{\Delta x_k^\top B_k \Delta x_k}
    $$

    目标是通过SMW公式推导其逆矩阵 $  H_{k+1} = B_{k+1}^{-1}  $。

    将BFGS更新公式视为对 $  B_k  $ 的秩2修正：

    $$
    B_{k+1} = B_k + U V^\top,\\ U = \begin{bmatrix} \frac{\Delta g_k}{\sqrt{\Delta g_k^\top \Delta x_k}} & -\frac{B_k \Delta x_k}{\sqrt{\Delta x_k^\top B_k \Delta x_k}} \end{bmatrix}, \quad V = \begin{bmatrix} \frac{\Delta g_k}{\sqrt{\Delta g_k^\top \Delta x_k}} & \frac{B_k \Delta x_k}{\sqrt{\Delta x_k^\top B_k \Delta x_k}} \end{bmatrix}
    $$

    根据Woodbury公式（SMW的秩k推广）：

    $$
    H_{k+1} = B_{k}^{-1} - B_{k}^{-1} U (I + V^\top B_{k}^{-1} U)^{-1} V^\top B_{k}^{-1}
    $$

    代入 $  H_k = B_{k}^{-1}  $，并化简后得到：

    $$
    H_{k+1} = H_k + \frac{\Delta x_k \Delta x_k^\top}{\Delta g_k^\top \Delta x_k} - \frac{H_k \Delta g_k \Delta g_k^\top H_k}{\Delta g_k^\top H_k \Delta g_k} + \frac{H_k \Delta g_k \Delta x_k^\top + \Delta x_k \Delta g_k^\top H_k}{\Delta g_k^\top \Delta x_k}
    $$
    将交叉项合并为对称形式：

    $$
    H_{k+1} = \left( I - \frac{\Delta x_k \Delta g_k^\top}{\Delta g_k^\top \Delta x_k} \right) H_k \left( I - \frac{\Delta g_k \Delta x_k^\top}{\Delta g_k^\top \Delta x_k} \right) + \frac{\Delta x_k \Delta x_k^\top}{\Delta g_k^\top \Delta x_k}
    $$
    将 $  H_{k+1}  $ 代入 $  H_{k+1} \Delta g_k = \Delta x_k  $：

    $$
    \begin{aligned}
    H_{k+1} \Delta g_k &= \left( I - \frac{\Delta x_k \Delta g_k^\top}{\Delta g_k^\top \Delta x_k} \right) H_k \left( I - \frac{\Delta g_k \Delta x_k^\top}{\Delta g_k^\top \Delta x_k} \right) \Delta g_k + \frac{\Delta x_k \Delta x_k^\top}{\Delta g_k^\top \Delta x_k} \Delta g_k \\
    &= \left( I - \frac{\Delta x_k \Delta g_k^\top}{\Delta g_k^\top \Delta x_k} \right) H_k \left( \Delta g_k - \Delta g_k \right) + \Delta x_k \\
    &= \Delta x_k
    \end{aligned}
    $$

    **验证通过**：BFGS更新公式满足拟牛顿条件。

  - Broyden族

    既然DFP和BFGS是互为对偶的，那用哪一个比较好呢？你当然可以通过若干组实验来测试哪个的性能的更优，或者对其收敛一通验证。但是一个比较的朴素的做法就是“我都要”，也就是取DFP迭代式和BFGS迭代式的正加权组合


- 最小二乘（超级经典问题）[Solving Non-linear Least Squares — Ceres Solver (ceres-solver.org)](http://ceres-solver.org/nnls_solving.html)

- Iteratively reweighted least squares (IRLS)

  **1. 问题定义**

  **目标**：求解线性最小二乘问题 $  \min_x \|Ax - b\|^2  $，并在新增数据点时**增量更新解** $  x  $，避免重新计算逆矩阵。

  **符号定义**：
  - $  A_k \in \mathbb{R}^{k \times n}  $: 前 $  k  $ 个样本的设计矩阵
  - $  b_k \in \mathbb{R}^k  $: 前 $  k  $ 个样本的观测向量
  - $  P_k = (A_k^\top A_k)^{-1}  $: 信息矩阵的逆（协方差矩阵）
  - $  x_k = P_k A_k^\top b_k  $: 第 $  k  $ 步的最小二乘解

  ---

  **2. 增量更新推导**

  当新增一个样本 $  (a_{k+1}, b_{k+1})  $ 时，设计矩阵和观测向量扩展为：

  $$
  A_{k+1} = \begin{bmatrix} A_k \\ a_{k+1}^\top \end{bmatrix}, \quad b_{k+1} = \begin{bmatrix} b_k \\ b_{k+1} \end{bmatrix}
  $$
  **2.1 更新信息矩阵逆 $  P_{k+1}  $**

  定义$  P_k = (A_k^\top A_k)^{-1}  $: 信息矩阵的逆（协方差矩阵），因为
  $$
  A_{k+1}^\top A_{k+1} = A_k^\top A_k +a_{k+1} a_{k+1}^\top
  $$
  根据Sherman-Morrison公式（秩1修正）：

  $$
  P_{k+1} = \left( A_k^\top A_k + a_{k+1} a_{k+1}^\top \right)^{-1} = P_k - \frac{P_k a_{k+1} a_{k+1}^\top P_k}{1 + a_{k+1}^\top P_k a_{k+1}}
  $$
  **2.2 更新参数估计 $  x_{k+1}  $**
  $$
  x_{k+1} = P_{k+1} A_{k+1}^\top b_{k+1} = P_{k+1} \left( A_k^\top b_k + a_{k+1} b_{k+1} \right)
  $$
  代入 $  P_{k+1}  $，得到参数更新公式：
  $$
  x_{k+1} = x_k + \frac{P_k a_{k+1}}{1 + a_{k+1}^\top P_k a_{k+1}} (b_{k+1} - a_{k+1}^\top x_k)
  $$
  ---

  **3. 算法步骤**

  **输入**：初始解 $  x_0  $, 初始逆矩阵 $  P_0 = \lambda^{-1} I  $（正则化项）
  **迭代流程**（对每个新样本 $  (a_{k+1}, b_{k+1})  $）：
  1. **计算预测残差**：

  $$
     e_{k+1} = b_{k+1} - a_{k+1}^\top x_k
   
  $$

  2. **计算增益向量**：

  $$
     K_{k+1} = \frac{P_k a_{k+1}}{1 + a_{k+1}^\top P_k a_{k+1}}
   
  $$

  3. **更新逆矩阵**：

  $$
     P_{k+1} = P_k - K_{k+1} a_{k+1}^\top P_k
   
  $$

  4. **更新参数估计**：

  $$
     x_{k+1} = x_k + K_{k+1} e_{k+1}
   
  $$
  ---

  **4. 复杂度分析**

  | 操作                   | 计算复杂度   |
  | ---------------------- | ------------ |
  | 增益向量 $  K_{k+1}  $ | $  O(n^2)  $ |
  | 更新 $  P_{k+1}  $     | $  O(n^2)  $ |
  | 更新 $  x_{k+1}  $     | $  O(n)  $   |

  总复杂度为 $  O(n^2)  $，优于直接求逆的 $  O(n^3)  $。

  ---

  **5. 扩展：Woodbury公式批量更新**

  若一次新增 $  m  $ 个样本 $  \{a_{k+1}^{(i)}, b_{k+1}^{(i)}\}_{i=1}^m  $，设：

  $$
  U = V = \begin{bmatrix} a_{k+1}^{(1)} & \cdots & a_{k+1}^{(m)} \end{bmatrix} \in \mathbb{R}^{n \times m}
  $$

  则Woodbury公式给出：

  $$
  P_{k+1} = P_k - P_k U (I + V^\top P_k U)^{-1} V^\top P_k
  $$

  适用于高吞吐量场景（如传感器网络）。

  **6. 数值稳定性**

  - **正则化**：初始 $  P_0 = \lambda^{-1} I  $ 避免 $  A^\top A  $ 奇异。
  - **数值误差控制**：定期重置 $  P_k  $ 或使用Cholesky分解更新。




hw：

- 复习最小二乘

- 整理+推导iterative least squares

  要求使用sherman-Morrison-Woodbarry

- BFGS，DFP公式证明（SMW推导）



## Lec 3

这节主要介绍以下几个方面

- Go over a little bit of the last lecture
- Focus on classical numerical optimizations
- References
  - MlT,theprincipleofoptimal control,thefirsttwolectures
  - Dive into deep learning, prepare for the later advanced optimizations in deeplearning
  - Lecturesforthe classtoday-Ryan的课程讲义，pdf文件另发



### review

- 对多元情况先做一元，二元找规律

****

设 $  A  \in R^{n\times n}为对称矩阵,b\in R^Pn,c\in R$ ，求：

1. 求线性函数 $  f(\mathbf{x}) = \mathbf{b}^\top \mathbf{x}  $ 的梯度和Hessian矩阵。

2. 给定二次函数：
   $$
   f(\mathbf{x}) = \mathbf{x}^\top A \mathbf{x} + \mathbf{b}^\top \mathbf{x} + c
   $$

   求其梯度和Hessian矩阵。

**Problem1** Gradient Hessian
$$
\frac{\partial f}{\partial x_k} = b_k \quad \Rightarrow \quad \nabla f(\mathbf{x}) = \mathbf{b}
$$

所有二阶偏导数为零：

$$
\nabla^2 f(\mathbf{x}) = 
\begin{bmatrix}
0 & 0 & \cdots & 0 \\
0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0
\end{bmatrix}
= \mathbf{0}
$$

**Problem2**

1. **展开函数**：



$$
   f(\mathbf{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j + \sum_{k=1}^n b_k x_k + c
 

$$

我们先来看$f(\mathbf{x}) = \mathbf{x}^\top A \mathbf{x}$这个部分
$$
f_1(\mathbf{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j
$$

单独对 $  x_k  $ 求偏导：
$$
\frac{\partial f_1}{\partial x_k} = \sum_{\substack{i=1 \\ i \neq k}}^n a_{ik} x_i + \sum_{\substack{j=1 \\ j \neq k}}^n a_{kj} x_j + 2a_{kk} x_k
\\
=\sum_{i=1}^{n}a_{ik}x_i +\sum_{j=1}^n a_{kj}x_j,~k = 1,2,...,n
$$

所以
$$
\nabla f_1(\mathbf{x}) =A^Tx+Ax =  2A\mathbf{x}(A~is~sysmetric)
$$

$$
\nabla^2 f_1(\mathbf{x})=2A
$$

对总体来说
$$
\frac{\partial f}{\partial x_k} = 2\sum_{i=1}^n a_{ki} x_i + b_k
$$

$$
\nabla f(\mathbf{x}) = 2A\mathbf{x} + \mathbf{b}
$$

二阶偏导数为常数：

$$
\frac{\partial^2 f}{\partial x_i \partial x_j} = 2a_{ij} \quad \Rightarrow \quad \nabla^2 f(\mathbf{x}) = 2A
$$

**Problem2** Jacobi

计算向量值函数 $\mathbf{F}(\mathbf{x}) = \begin{pmatrix} f_1(\mathbf{x}) \\ f_2(\mathbf{x}) \end{pmatrix}$ 在点 $\mathbf{x} = (1, 0, \pi)^\top$ 处的雅可比矩阵，其中：
- $f_1(\mathbf{x}) = 3x_1 + e^{x_2} x_3$
- $f_2(\mathbf{x}) = {x_1^3 + x_2 \sin x_3}$

- 对 $f_1$ 求偏导：


$$
     \begin{cases}
     \dfrac{\partial f_1}{\partial x_1} = 3 \\
     \dfrac{\partial f_1}{\partial x_2} = e^{x_2}x_3 \\
     \dfrac{\partial f_1}{\partial x_3} = e^{x_2}
     \end{cases}
   

$$

   - 对 $f_2$ 求偏导：



$$
\begin{cases}
     \dfrac{\partial f_2}{\partial x_1} = 3x^1 \\
     \dfrac{\partial f_2}{\partial x_2} = 2x_2\sin{x_3} \\
     \dfrac{\partial f_2}{\partial x_3} = x_2^2\cos{x_3}
     \end{cases}
$$

**代入点 $\mathbf{x} = (1, 0, \pi)$**：
$$
\mathbf{J_F}(1, 0, \pi) = \begin{pmatrix}
3 & \pi & 1 \\
3 & 0 & 0
\end{pmatrix}
$$

### Mathematical Fundamentals

- “形式”很重要：先有鸡还是先有蛋？先接受这个形式再最优化求解

**方向导数定义**：方向导数就是函数值在某个“**方向**”上的变化率。

函数$f(x)$在x点关于方向d的方向导数，设 $\Phi(a) = f(x + ad)$，令$u=x+ad$
$$
u=(x_1+ad_1,...,x_n+ad_n)^T = (u_1,...u_n)^T
$$

$$
\Phi\prime(a) = \frac{\partial f(u)}{\partial u_1}\frac{du_1}{da}+...+\frac{\partial f(u)}{\partial u_n}\frac{du_n}{da}\\
 = [\nabla f(u)]^Td = [\nabla f(x+ad)]^Td = d^T[\nabla f(x+ad)]
$$

> $<a,b> = <b,a>$

则一阶方向导数：
$$
\Phi'(0) = \lim_{a \to 0} \frac{f(x+ad) - f(x)}{a} = \nabla f(x) \cdot d
$$

$$
\nabla f(x) \cdot d = \sum_{i=1}^n \frac{\partial f(x)}{\partial x_i} d_i = \begin{bmatrix} \frac{\partial f}{\partial x_1} & \cdots & \frac{\partial f}{\partial x_n} \end{bmatrix}
\begin{bmatrix} d_1 \\ \vdots \\ d_n \end{bmatrix}
$$

二阶方向导数可通过继续求导得到：
$$
\Phi''(a) = \sum_{i=1}^n \left[ \sum_{j=1}^n \frac{\partial^2 f(u)}{\partial u_i \partial u_j} d_j \right] d_i
$$

$$
\Phi''(a) = \begin{bmatrix} d_1 & \cdots & d_n \end{bmatrix}
\begin{bmatrix}
\frac{\partial^2 f}{\partial u_1^2} & \cdots & \frac{\partial^2 f}{\partial u_1 \partial u_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial u_n \partial u_1} & \cdots & \frac{\partial^2 f}{\partial u_n^2}
\end{bmatrix}
\begin{bmatrix} d_1 \\ \vdots \\ d_n \end{bmatrix}\\
 = d^\top \nabla^2 f(x+a) \ d
$$

$$
\Phi''(0) = d^\top \nabla^2 f(x) \ d
$$

----



对于任意给定的$d\ne 0$，若极限$\lim_{a \to 0^+} \frac{f(\mathbf{\bar x} + ad) - f(\mathbf{\bar x})}{a||d||}$记为$\frac{\partial}{\partial d}f(\bar x)$，即
$$
\frac{\partial}{\partial d}f(\bar x)=\lim_{a \to 0^+} \frac{f(\mathbf{\bar x} + ad) - f(\mathbf{\bar x})}{a||d||}
$$
**定理一**：若函数$f(x)$具有连续一阶偏导数，则它在$\bar x$处沿方向d的一阶方向导数为
$$
\frac{\partial}{\partial d}f(\bar x) =<\nabla f,\frac{d}{||d||}>=\frac{1}{\|\mathbf{d}\|}\mathbf{d}^T \nabla f
$$
证明如下：

对于任意非零方向向量 $\mathbf{d} = (d_1, d_2, \ldots, d_n)^\top$，函数 $f(\mathbf{x})$ 在点 $\mathbf{x} = (x_1, x_2, \ldots, x_n)^\top$ 处的方向导数定义为：

$$
D_{\mathbf{d}} f(\mathbf{x}) = \lim_{a \to 0^+} \frac{f(\mathbf{x} + a\mathbf{d}) - f(\mathbf{x})}{a \|\mathbf{d}\|}
$$
1. **定义辅助函数**：

$$
   \phi(a) = f(\mathbf{x} + a\mathbf{d})
$$

   其中 $\phi(a)$ 是关于标量 $a$ 的一元函数。

2. **对 $\phi(a)$ 求导**：

$$
\phi'(a) = \frac{d}{da} f(\mathbf{x} + a\mathbf{d}) = \mathbf{d}^\top \nabla f(\mathbf{x} + a\mathbf{d})
$$
3. **计算 $a=0$ 处的导数**：

$$
   \phi'(0) = \mathbf{d}^\top \nabla f(\mathbf{x})
$$
4. **方向导数表达式推导**：

$$
D_{\mathbf{d}} f(\mathbf{x}) = \frac{1}{\|\mathbf{d}\|} \lim_{a\to0^+} \frac{\phi(a)-\phi(0)}{a}
=
\frac{1}{\|\mathbf{d}\|} \phi'(0) = \frac{1}{\|\mathbf{d}\|} \mathbf{d}^\top \nabla f(\mathbf{x})
$$
由Cauchy-Sehwarz不等式可得
$$
-||\nabla f(\bar x)||\le \frac{\partial}{\partial d}f(\bar x)\le ||\nabla f(\bar x)||
$$
$d=-||\nabla f(\bar x)$为最速下降方向

定理二：f(x)在x沿方向d的二阶方向导数为：
$$
\frac{\partial}{\partial d}f(\bar x)=\lim_{a \to 0^+} \frac{\frac{\partial}{\partial d}f(\mathbf{\bar x} + ad) - \frac{\partial}{\partial d}f(\mathbf{\bar x})}{a||d||}=\frac{1}{||d||^2}d^T \nabla^2f(\bar x)d
$$

----

**鞍点与焦点**

- 焦点（focus）：在动力系统的相平面分析中，**焦点**是一类**平衡点**（即系统状态变化率为零的点），其周围轨迹呈现螺旋状收敛或发散的特征。焦点分为**稳定焦点**和**不稳定焦点**两种类型
- 鞍点（saddle point）







SQP,

# CMU-16-745

## Lec 1 系统状态方程、平衡点与稳定性

Optimal Control and RL are the same thing

- 连续系统状态方程（Continuous Time Dynamics）
- 仿射系统状态方程（Control-Affine System）
- 机械臂系统状态方程（Manipulator Dynamics）
- 线性系统
- 平衡点（Equilibria）
- 平衡点的稳定性（Stability）

[Lecture 1 系统状态方程、平衡点与稳定性 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/629135263)



## Lec 2 离散状态方程、数值积分与稳定性

- 状态方程离散
  - 离散状态方程
  - 稳定性分析
  - 案例分析
    - 前向欧拉积分
    - 龙格库塔法RK4
    - 后向欧拉积分
- 控制量的离散

[Lecture 2 离散状态方程、数值积分与稳定性 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/629135862)

non-causal非因果关系

- 要小心离散情况下的ODE，特别是对于临界的情况，最好做sanity check（检查energy）



## Lec 3 求根法与无约束的最优化问题

- 符号约定（Notation）
- 方程求根（Root Finding）
  - 牛顿法（Newton's method）
  - 不动点迭代法Fixed point Iteration）
- 最小化问题（Minimization）
  - 充分条件与必要条件
  - 正则化（regularization）
  - 线搜索（line search）

[Lecture 3 求根法与无约束的最优化问题 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/629137277)

在牛顿法中，若Hessian矩阵正定，Cholesky分解可快速计算 ( $H^{-1}$ )，避免直接求逆。并且也可以通过分解判断是否正定，如果不正定使用阻尼牛顿法

## Lec 4 约束最优化问题

- 等式约束
  - 一阶必要条件与牛顿法
  
  - 高斯牛顿法
  
    高斯牛顿法在实际中往往比较常用，因为每次迭代比较快，而且具有**超线性**的收敛性
  
- 不等式约束
  - 一阶必要条件（KKT条件）
  - [Active-Set法](https://zhida.zhihu.com/search?content_id=227940219&content_type=Article&match_order=1&q=Active-Set法&zhida_source=entity)
  - [障碍函数法](https://zhida.zhihu.com/search?content_id=227940219&content_type=Article&match_order=1&q=障碍函数法&zhida_source=entity)/[内点法](https://zhida.zhihu.com/search?content_id=227940219&content_type=Article&match_order=1&q=内点法&zhida_source=entity)
  - [罚函数法](https://zhida.zhihu.com/search?content_id=227940219&content_type=Article&match_order=1&q=罚函数法&zhida_source=entity)
  - 增广拉格朗日法
  
- 二次规划QP

[Lecture 4 约束最优化问题 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/629139142)

heuristic启发式

机器人QP问题：将机器人实际需求（如关节运动、力分配、轨迹跟踪）转化为**带约束的二次优化问题**，通过求解该问题获得满足物理限制的最优解

二次规划（QP）问题的标准形式

**目标函数**：
$$
\min_x \quad \frac{1}{2} x^T H x + c^T x
$$

**约束条件**：
1. **等式约束**：
$$
A x = b
$$

2. **不等式约束**：
$$
lb \leq x \leq ub
$$

---

变量说明

- $  x  $：优化变量（如关节力矩、足底力）。
- $  H  $：正定矩阵，保证问题为凸优化，有唯一解。
- $  A  $：等式约束矩阵。
- $  b  $：等式约束向量。
- $  lb  $、$  ub  $：变量的下界和上界。

**QP问题的求解方法**

1. **求解器类型**：
   - **Active-set方法**（如qpOASES）：适合中小规模问题，实时性强，常用于嵌入式系统。
   - **内点法**（如IPOPT、OSQP）：适合大规模问题，稳定性高。
   - **凸优化库**（CVXPY、CasADi）：提供建模接口，简化问题构建。
2. **机器人中的实际使用**：
   - **MIT Cheetah**：使用qpOASES实时求解足底力分配。
   - **工业机械臂**：通过OSQP生成无碰撞轨迹。
   - **无人机集群**：利用CVXPY建模多机协同避障。



## Lec 5 带约束线搜索与正则化

- [对偶性](https://zhida.zhihu.com/search?content_id=227940583&content_type=Article&match_order=1&q=对偶性&zhida_source=entity)与[正则化](https://zhida.zhihu.com/search?content_id=227940583&content_type=Article&match_order=1&q=正则化&zhida_source=entity)（Regularization and Duality）
- 指标函数与线搜索（Merit Function）
- 带约束最小化问题

[Lecture 5 带约束线搜索与正则化 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/629140808)



## Lec 6 确定性最优控制

- 控制简史
- 确定性最优控制（Deterministic Optimal Control）
  - 连续时间
  - 离散时间
- [极小值原理](https://zhida.zhihu.com/search?content_id=227941610&content_type=Article&match_order=1&q=极小值原理&zhida_source=entity)（Pontryagin's Minimum Principle）

[Lecture 6 确定性最优控制（Deterministic Optimal Control） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/629145483)

前面Lecture 1-5我们一直在打基础，学习了如何离散化模型，如何做一些优化，从这一节开始，我们正式进入最优控制的学习

最后，教授总结，当前研究的困难点有

- 如何找到通用的处理接触的理论
- 如何将model-base控制和model-free的强化学习结合
- 如何在强化学习中加入先验知识让RL更加数据高效
- 如何保证不确定非线性系统的控制安全裕度
- 在一个非协作的环境中，如何处理其他adversaial 课题



确定性最优控制是指系统的模型是确定的，在当前状态x给一个特定的u，根据系统方程我就一定可以知道下一步状态会怎么转移。

- 通过解这个最优化解出来的轨迹是一个开环轨迹**（因此，要么就解的特别快，总是用前面的开环轨迹执行，如MPC问题，要么就离线解，在线用一个很好的反馈控制器来跟踪）**。与此相反，在**随机最优控制问题**中，由于问题的定义是充满噪声的，因此解随机最优控制问题必须要得到一个闭环的轨迹。
- (1)式指定的优化问题在大部分时候，是没有解析解的，但是少部分很特殊的情况有，如LQR问题。



极小值原理（Pontryagin's Minimum Principle）

KKT条件[Karush-Kuhn-Tucker (KKT)条件 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/38163970)



## Let 7 LQR（Linear Quadratic Regulator）的三种解法

- 间接法（indirect method）、打靶法（shooting method）
- 二次规划[QP解法](https://zhida.zhihu.com/search?content_id=227941801&content_type=Article&match_order=1&q=QP解法&zhida_source=entity)
- [Riccati迭代](https://zhida.zhihu.com/search?content_id=227941801&content_type=Article&match_order=1&q=Riccati迭代&zhida_source=entity)(Riccati Recursion)

[Leture 7 LQR（Linear Quadratic Regulator）的三种解法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/629146365)

[LQR解析解推导：从LQR到迭代Riccati方程 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/636305927)

Riccati方程的核心应用在于**最优控制与状态估计**，具体包括：

1. **线性二次调节器（LQR）**： 设计反馈控制器，最小化二次型性能指标 J=∫(x⊤Qx+u⊤Ru)dt，通过求解CARE得到最优反馈矩阵 K=R−1B⊤P，使闭环系统稳定。
2. **卡尔曼滤波（LQG）**： 在含噪声的系统中估计状态，通过DARE求解最优滤波器增益，最小化估计误差协方差。
3. **经济与金融优化**： 处理时间不一致控制问题，如均衡Riccati方程用于动态资源分配。

[(22 封私信 / 80 条消息) 请详细介绍下黎卡提方程在控制理论中的重要性和作用？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/20081582)

标准的代数Riccati方程分为如下两种：

- 连续时间代数Riccati方程（CARE）：
- 离散时间代数Riccati方程（DARE）



**哈密顿矩阵法**?



# References

- https://zhuanlan.zhihu.com/p/629131647
- [2023-2024春夏许超老师最优化与最优控制课程分享 - CC98论坛](https://www.cc98.org/topic/5923253)

- [Tutorial — Ceres Solver (ceres-solver.org)](http://ceres-solver.org/tutorial.html)