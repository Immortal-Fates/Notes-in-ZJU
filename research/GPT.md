# Main Takeaway
NLP(Natural Language Processing)

# BERT



# GPT

## GPT1
- self supervised
  - unsupervised pre-training: transformer decoder
  - supervised fine-tuning

## GPT2

- bigger and larger
- add zero-shot
  - Receives no labeled examples for a task
  - Receives only instructions or descriptions of the task
  - Must complete the task using knowledge learned from other data 


# References

- [GPT，GPT-2，GPT-3 论文精读【论文精读】]( https://www.bilibili.com/video/BV1AF411b7xQ/?share_source=copy_web&vd_source=93bb338120537438ee9180881deab9c1)
