# Sequence Models and Attention Mechanism

- **Long Short-Term Memory**. Hochreiter Sepp et.al. **Neural Computation**, **1997-11-1**, ([pdf](..\..\..\papers\models\lstm.pdf))([link](https://doi.org/10.1162/neco.1997.9.8.1735)).
- **Learning Phrase Representations using RNN Encoder-Decoder for
  Statistical Machine Translation**. Kyunghyun Cho et.al. **arxiv**, **2014**, ([pdf](..\..\..\papers\models\GRU.pdf))([link](http://arxiv.org/abs/1406.1078v3)).
- **Attention Is All You Need**. Ashish Vaswani et.al. **arxiv**, **2017**, ([pdf](..\..\..\papers\models\Attention_Is_All_You_Need.pdf))([link](http://arxiv.org/abs/1706.03762v7)).
- **BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding**. Jacob Devlin et.al. **arxiv**, **2018**, ([pdf](..\..\..\papers\models\BERT.pdf))([link](http://arxiv.org/abs/1810.04805v2)).
- **Improving Language Understanding
 by Generative Pre-Training**. ([pdf](..\..\..\papers\models\GPT-1.pdf)).



# Cmd

```
set PYTHONUTF8=1
autoliter -i ./Sequence_Models_and_Attention_Mechanism.md -o ../../../papers/models/
```