# Lec 深度学习与图像处理

图像处理应用案例讲解

## 概述

计算机视觉是一门研究如何使机器“看懂”这个世界的科学，通过神经网络完成视觉任务

- 图像生成、迁移
- 文字生成图像、视频(Sora)
- 卷积神经网络（CNN）成为核心算法，全连接网络

## 图像分类

- One-hot向量：采用向量形式表示类别，独热编码，向量的维度等于类别的数量

- Softmax：用于多类别分类的激活函数
  $$
  \sigma(z)_j = \frac{e^{z_j}}{\sum e^{z_k}}
  $$
  将一个实数值向量压缩到(0,1)，所有元素和为1

- 交叉熵损失（Cross Entropy Loss），衡量两个模型之间的差异
  $$
  H(p, q) = -\sum_{x_i} p(x_i) \log q(x_i) \\
  p: label, one-hot向量\\
  q: 预测, 概率分布
  $$

![image-20250610150016973](assets/数字图像处理Lec13_16.assets/image-20250610150016973.png)

## 图像检索

- 定义：图像检索就是通常意义上的**以图搜图**，给定一张图像，从图库中找出跟它匹配的图像，并按相似度进行排序，越相似的越靠前

  ![image-20250610150106147](assets/数字图像处理Lec13_16.assets/image-20250610150106147.png)

- 基本流程

  ![image-20250610150125146](assets/数字图像处理Lec13_16.assets/image-20250610150125146.png)

下面举一个常见的图像检索的例子——行人重识别系统。常需要经过三个步骤

1. 特征提取：学习能够应对在不同摄像头下行人变化的特征——表征学习

   - 分类损失（ID损失）：利用行人的ID作为训练标签来训练模型，每次只需要输入一张图片

     行人ID数位网络的类别数，特征层后接一个分类FC，经过Softmax激活函数计算交叉熵损失。**测试阶段**使用倒数第二层的特征向量进行检索，分类FC层丢弃

   - 验证损失：输入一对（两张）行人图片，让网络来学习这两张图片是否属于同一个行人，等效于二分类问题

     融合两个特征信息计算一个二分类损失（验证损失），训练阶段可以和ID损失一起使用。**测试阶段**输入两张图片，直接判断该两张图片是否属于一个行人

   特征：

   - 通常需要额外的FC层来辅导特征学习，测试阶段FC层会被丢弃
   - ID损失的FC层维度与ID数量一致，当训练集太大时网络巨大，训练收敛很难

2. 度量学习：将学习到的特征映射到新的空间使相同的人更近，不同的人更远

   定义一个映射
   $$
   f(x): \mathbb{R}^F \rightarrow \mathbb{R}^D
   $$
   将图片从原始域映射到特征域，之后再定义一个距离度量函数
   $$
   D(x, y): \mathbb{R}^D \times \mathbb{R}^D \rightarrow \mathbb{R}
   $$
   来计算两个特征向量之间的距离。最后通过最小化网络的度量损失，来寻找一个最优的映射 $f(x)$，使得相同行人两张图片（正样本对）的距离尽可能小，不同行人两张图片（负样本对）的距离尽可能大。而这个映射 $f(x)$ 就是我们训练得到的深度卷积网络。

   **三元组损失**（Triplet Loss）是深度度量学习（Deep Metric Learning）中的核心损失函数，专为学习**判别性特征嵌入**而设计。其核心思想是：通过同时输入**锚点样本（Anchor）**、**正样本（Positive）** 和**负样本（Negative）**，强制模型拉近同类样本距离，推远异类样本距离。

   输入结构：三元组（Anchor基准样本, Positive, Negative）

   难样本挖掘：我们可以挑选一个最难的正样本和一个最难的负样本和a组成一个三元组

   特征：

   - 网络大小与训练集规模无关，但是数据采样器时间消耗会增加
   - Trihard Loss 为目前业界度量学习的标杆

3. 图像检索：根据图片特征之间的距离进行排序，返回检索结果

   常用的评价指标

   - rank-k：算法返回的排序列表中，前k位为存在检索目标则称为rank-k命中
   - CMC曲线（累计匹配曲线）：计算rank-k的击中率，形成rank-acc的曲线
   - mAP曲线(mean average precision)：反映检索的人在数据库中所有正确的图片排在排序列表前面的程度，能更加全面的衡量ReID算法的性能

   ![image-20250610150412270](assets/数字图像处理Lec13_16.assets/image-20250610150412270.png)

## 目标检测

- 检测任务介绍

  - PASCAL VOC 数据集，COCO 数据集

  - IOU 指标（Intersection over Union）
    $$
    IOU = \frac{DetectionResult \cap GroundTruth}{DetectionResult \cup GroundTruth}
    $$

  - AP指标：对于每一类的样本，按照给定的IOU阈值，用预测概率对类别做预测，按照置信度从大到小排列，然后分别计算Precision、Recall就可以做出P-R曲线，曲线下面所围成的面积就是AP（Average-Precision）

  - mAP指标：平均精度均值（Mean Average Precision , mAP）：即把每个类别的AP都单独拿出来，然后计算所有类别AP的平均值，代表着对检测到的目标平均精度的一个综合度量。

- 经典算法

  - 引入CNN的R-CNN

    - 使用了Selective Search。从一张图像生成约2000-3000个候选区域。（耗时）
    - 然后计算CNN特征，使用得到的特征进行SVM分类。
    - 最后进行bounding box 回归

  - Fast R-CNN

    R-CNN的问题：生成region proposal非常耗时，速度极慢；不能端到端（end2end）训练

    - Fast R-CNN使用一个卷积神经网络对全图进行特征提取

    - 提出ROI pooling layer来统一到相同的大小，即提取一个固定维度的特征表示

      > ROI：region of interest

    - 统一多任务损失函数，实现end2end训练

  - Faster R-CNN

    由共享卷积层、RPN、RoI pooling以及分类和回归四部分组成：

    ![image-20250610160908577](assets/数字图像处理Lec13_16.assets/image-20250610160908577.png)

    1. 首先使用共享卷积层为全图提取特征 feature maps
    2. 将得到的 feature maps 送入 RPN，RPN 生成待检测框（指定 RoI 的位置），并对 RoI 的包围框进行第一次修正
    3. RoI Pooling Layer 根据 RPN 的输出在 feature map 上面选取每个 RoI 对应的特征，并将维度置为定值
    4. 使用全连接层（FC Layer）对框进行分类，并且进行目标包围框的第二次修正

    特点：

    - 提出 Region Proposal Network（RPN）来进行Proposal提取。
    - 提出Anchor boxes（锚框）
    - 速度极大提升

  - YOLO：you only look once

    算法步骤：

    1. 将图像Resize到448x448
    2. 图像划分格点（7x7) 
    3. 运行CNN
    4. 非极大抑制优化检测结果

    优缺点：

    - 检测物体非常快
    - 小目标检测效果差
    - 损失函数的设计对坐标的回归和分类的问题同时用MSE损失不合理
    - YOLO直接预测的BBox位置，相较于预测物体的偏移量，模型会不太好稳定收敛

  - SSD(Single Shot MultiBox Detector)

    - 直接使用feature map进行预测。浅层feature map预测小物体，深层预测大物体。
    - 采用了FasterRCNN中的Anchors设计。

  几个对比

  ![image-20250610161437862](assets/数字图像处理Lec13_16.assets/image-20250610161437862.png)

- 目标检测前沿

  https://github.com/hoya012/deep_learning_object_detection
  https://github.com/amusi/awesome-object-detection

## 图像分割

- 基本介绍

  - 深度学习中的分割：将图像按照语义类别分为多个子区域，实际上是给图像上的每一个像素预测一个标签
  - 语义分割（Semantic Segmentation）：一类物体占有一种标签
  - 实例分割（Instance Segmentation）：每一个独立的物体占有一种标签
  - 评价指标
    1. 像素精度（pixel accuracy）：每一类像素正确分类的个数 / 每一类像素的实际个数
    2. 均像素精度（mean pixel accuracy）：每一类像素的精度的平均值
    3. 平均交并比（Mean Intersection over Union）：每一类的 IOU 取平均值（最常用）
    4. 权频交并比（Frequency Weight Intersection over Union）：每一类出现的频率作为权重

- 模型发展历程

  1. 开山之作：FCN: 《Fully Convolutional Networks for Semantic Segmentation》CVPR2015 可以说近年来的所有分割模型都是以FCN结构为基本模型，有以下两个关键点

     1. 把全连接层换成卷积层，使得模型以滑窗的形式处理各个像素的分类
     2. 不同层级信息的融合，使得预测结果兼顾高层语义信息和底层位置信息

     ![image-20250610164558085](assets/数字图像处理Lec13_16.assets/image-20250610164558085.png)

     前面的层：低级别语义信息+高级别位置信息
     后面的层：高级别语义信息+低级别位置信息

  2. 《SegNet: A Deep Convolutional Encoder-Decoder Architecturefor Image Segmentation 》TPAMI 2015

     - Encoder-Decoder 结构
     - 带有index的上采样

  3. U-Net：

     - Encoder-Decoder 结构

     - 更加丰富的特征融合

       > 区别于FCN, U-net的特征融合用的是拼接（concat）

  4. Deeplab系列： V1、V2、V3、V3+

     Receptive field (RF, 感受野) 解决方案：降采样（Strided convolution）

     ![image-20250610164902629](assets/数字图像处理Lec13_16.assets/image-20250610164902629.png)

     如果希望图像中各自远离的两像素点建立联系，需要进行多次卷积操作

  5. DFN:《Learning a Discriminative Feature Network for Semantic Segmentation》CVPR2018

- 常用技巧

  - 感受野控制与特征融合
    1. 跨层连接（skip-connection）：FCN、RefineNet、U-net
    2. 空间金字塔池化（Spatial Pyramid Pooling） : SPPnet
    3. 带孔卷积（Dilated convolution） : ASPP、HDC(Hybrid Dilated Conv)、 Dense-ASPP
  - 模型提速
    1. 简化模型参数：MobileNet、ShuffleNet结构；最好避免使用Dilated Conv
    2. 在允许精度下缩小输入图像尺寸：最直接的方法
    3. 底层特征与高层特征分开： ICnet 、 BiSeNet

- 前沿课题

  - Segment Anything Model (SAM)



## 姿态估计

- 2D 姿态估计（2D Pose Estimation）

  - 位置描述：关键点的坐标$(x,y)$，heat map

  - 使用卷积神经网络回归热力图 ——效果好精度高

  - 主流方法

    - Top-down: 先用检测方法框出单个人，再预测关键点
      - 优点：在检测准确的情况下精度较高，不同人之间的干扰少
      - 缺点：检测一旦精度不高，关键点定位精度也不可能好；随着图片中人的增多，检测部分耗时长，实时性难以满足
    - Bottom-up：先检测出一张图的所有关键点，再分配到个人
      - 优点：去掉了检测流程，在分配算法优良的情况下实时性较高
      - 缺点：不同人关键点之间的干扰大，需要好的关键点解析和分配方法

  - 经典姿态估计网络

    - Convolutional Pose Machine （CPM）

      特点：Refine 机制，鲁棒性高；中继监督训练，避免过深网络难以优化的问题

      > Refine机制（级联细化机制）：显式融合原始图像特征 + 前一阶段热图

      效果：

      - CPM的因其refine机制鲁棒性很好，域迁移能力强，可以很容易迁移到其他关键点定位的任务上
      - CPM缺少很强的多尺度信息融合的特性，依赖数据增广，实践证明给CPM加入多尺度特性（FPN）可以提高其性能
      - CPM的“脑补”能力强，在遮挡的情况下也能取得不错的效果

    - Stacked Hourglass

    - Openpose：使用Bottom-up方法，使用CPM网络

      主要创新点

      - 网络有两个分支，第一个分支回归heatmap位置，第二个分支回归表征关节链接向量的亲和区域（PAF）
      - 利用图论中的二分图理论，采用匈牙利算法求得相连关键点最优匹配（最大匹配：匹配边数最多 ）



## 点云处理

- PointNet: 三维点云的目标识别与分割
- 神经辐射场（NeRF）





