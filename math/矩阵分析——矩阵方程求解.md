# Main Takeaway

本节介绍《矩阵分析与应用》——张显达，第六章，矩阵方程求解

<!--more-->

# 矩阵方程求解

矩阵方程求解是优化课程之后的一个例子，求解：
$$
Ax = b ~~ or ~~ Ax\approx b
$$
这一章就是如果$Ax= b$有解那就解出来，如果无解，那就找一个最优的近似解

然后我们需要判断$Ax = b$有没有解：

> $A$为$m\times n$的数据矩阵，$x$位$1\times n$未知权重，$b$为$m\times 1$数据向量

- $m>n$：超定，可能无解

  $rank([A,b])>rank(A)$，b无法被A列线性组合表示，无解

- $m< n$：欠定，可能无穷组解
  $$
  rank([A,b])=rank(A)  \begin{cases}=n&唯一解 \\>n& 无穷多解 \end{cases}
  $$

## 最小二乘方法

- 最小二乘(Least-Squares，LS)
  $$
  Ax \approx b \\
  Ax = b + \Delta b \quad \text{校正向量}
  $$

  $$
  \min_x \|\Delta b\|_2^2 = \|Ax - b\|_2^2 = (Ax - b)^T(Ax - b)
  $$

  > 为什么这里要选择二范数呢？
  >
  > 这是因为Gauss-Markov定理，它确立了在特定条件下，**普通最小二乘法**（Ordinary Least Squares, OLS）估计量在所有线性无偏估计量中的最优性——最小方差的无偏估计，下面给出推导
  >
  > 考虑线性回归模型：
  >
  > $$
  > y = X\beta + \epsilon
  > $$
  > 其中：
  > - $  X  $ 是满秩的设计矩阵（$  \text{rank}(X) = p  $）。
  > - $  \epsilon  $ 是误差向量，满足 $  E[\epsilon] = 0  $ 和 $  \text{Var}(\epsilon) = \sigma^2 I_n  $。
  >
  > 一个线性估计量 $  \hat{\beta}  $ 可以表示为：$$\hat{\beta} = C y$$
  >
  > 其中 $  C  $ 是一个依赖于设计矩阵 $  X  $ 的矩阵。为了保证无偏性，我们需要：
  >
  > $$
  > E[\hat{\beta}] = E[C y] = C E[y] = C (X\beta) = \beta
  > $$
  > 这意味着：$$C X = I_p$$
  >
  > 接下来，我们计算线性无偏估计量 $  \hat{\beta} = C y  $ 的方差：
  >
  > $$
  > \text{Var}(\hat{\beta}) = \text{Var}(C y) = C \text{Var}(y) C^T = C (\sigma^2 I_n) C^T = \sigma^2 C C^T
  > $$
  > 由于 $  C X = I_p  $，我们可以表示 $  C  $ 为：
  >
  > $$
  > C = (X^T X)^{-1} X^T + D
  > $$
  > 其中 $  D  $ 是一个满足 $  D X = 0  $ 的矩阵。这是因为：
  >
  > $$
  > C X = [(X^T X)^{-1} X^T + D] X = I_p + D X = I_p
  > $$
  > 因此，$  D X = 0  $
  >
  > 现在我们计算估计量的方差：
  >
  > $$
  > \text{Var}(\hat{\beta}) = \sigma^2 C C^T = \sigma^2 [(X^T X)^{-1} X^T + D][(X^T X)^{-1} X^T + D]^T
  > $$
  > 展开后得到：
  >
  > $$
  > \text{Var}(\hat{\beta}) = \sigma^2 [(X^T X)^{-1} X^T X (X^T X)^{-1} + (X^T X)^{-1} X^T D^T + D X (X^T X)^{-1} + D D^T]
  > $$
  > 由于 $  D X = 0  $，所以：
  >
  > $$(X^T X)^{-1} X^T D^T = 0 \quad \text{和} \quad D X (X^T X)^{-1} = 0$$
  >
  > 因此，方差简化为：
  >
  > $$
  > \text{Var}(\hat{\beta}) = \sigma^2 [(X^T X)^{-1} + D D^T]
  > $$
  > 由于 $  D D^T \geq 0  $（半正定矩阵），因此：
  >
  > $$
  > \text{Var}(\hat{\beta}) \geq \sigma^2 (X^T X)^{-1}
  > $$
  > 当且仅当 $  D = 0  $ 时，等号成立。这意味着：
  >
  > $$
  > C = (X^T X)^{-1} X^T
  > $$
  > 这正是 **普通最小二乘法**（OLS）估计量的系数矩阵。因此，OLS估计量在所有线性无偏估计量中具有 **最小方差**。
  >

- 普通最小二乘解
  $$
  \hat{x}_{LS} = \arg \min_x \|Ax - b\|_2^2 \\
  \|Ax - b\|_2^2 \triangleq f(x) \\
  = x^T A^T A x - x^T A^T b - b^T A x + b^T b
  $$
  梯度：$$\nabla_x f(x) = 2A^T A x - 2A^T b = 0$$

  Hessian：$$\nabla^2 f(x) = 2A^T A \quad \text{半正定，凸函数}$$

  我们求解平稳点，即倒数为零得到如下方程
  $$
  A^T A x = A^T b
  $$

  - A：$  m \geq n  $, $  \text{rank}(A) = n  $, $  A^T A  $ 可逆
    $$
    x_{LS} = (A^T A)^{-1} A^T b
    $$

  - $  m \geq n  $, $  \text{rank}(A) < n  $, 伪逆
    $$
    x_{LS} = (A^T A)^\dagger A^T b
    $$

  - 欠定方程 $  \text{rank}(A) = m < n  $, 不可辨识

- 数据最小二乘

  前面是对b的误差进行讨论，现在对A的误差进行讨论
  $$
  Ax = b
  $$
  数据矩阵 $  A  $ 的误差

  $$
  A = A_0 + E \quad \text{误差矩阵} \\
  E_{ij} \overset{iid}{\sim} N(0, \sigma^2)
  $$
  取校正矩阵 $  \Delta A  $，满足目标

  $$
  (A + \Delta A)x = b \\
  \hat{x}_{\text{DLS}} = \arg \min_x \|\Delta A\|_F^2 \quad \text{s.t. } (A + \Delta A)x = b
  $$

  对于上述带等式约束的优化问题，使用Lagrange 乘数法:

  $$
  L(\Delta A, x, \lambda) = \|\Delta A\|_F^2 + \lambda^H [(A + \Delta A)x - b] \\
  = \text{tr}(\Delta A \Delta A^H) + \lambda^H [(A + \Delta A)x - b]
  $$
  求解：
  $$
  \begin{cases}0 = \frac{\partial L}{\partial \Delta A} = \Delta A + \lambda x^H = 0 \Rightarrow \Delta A = -\lambda x^H \\0 = \frac{\partial L}{\partial \lambda^H} = (A + \Delta A)x - b\end{cases}\Rightarrow\begin{cases}\lambda = \frac{Ax - b}{x^H x} \\\Delta A = -\frac{(Ax - b)x^H}{x^H x}\end{cases}
  $$
  代入原式得到
  $$
  \min_x J(x) = \frac{(Ax - b)^H (Ax - b)}{x^H x}
  $$
  对于新的代价函数可以使用下面两种方法来进行求解：

  1. 梯度下降法: $  x^{t+1} = x^t - \gamma_t \nabla J(x)  $
  2. 分式优化的结构化方法

## Tikhonov正则化与正则Gauss-Seidel法

- Tikhonov正则化

  普通最小二乘的求逆可能矩阵是病态的，于是我们加上单位阵，下面我们来看看修改之后对应的解与原来解的区别：
  $$
  \min_x \|Ax - b\|_2^2 \quad \rightarrow \quad \min_x J(x) = \min_x \|Ax - b\|_2^2 + \lambda \|x\|_2^2, \lambda \geq 0 \\
  \hat{x}_{LS} = \left( A^T A \right)^{-1} A^T b \quad \rightarrow \quad \hat{x}_{Tik} = \left( A^H A + \lambda I \right)^{-1} A^H b
  $$

  > 对改变后的问题求解可以得到加上正则化后的解

  opt modeling: $ \min_x \text{loss}(x) + \lambda \text{regularizer}(x) $

  $$
  \begin{array}{|c|}\hline\text{data fidelity term} \\\text{数据保真项} \\\hline\end{array}\quad\begin{array}{|c|}\hline\text{prior knowledge} \\\text{先验知识 | 神谕} \\\hline\end{array}
  $$

## ML and LS

最大似然估计和最小二乘的可转换性

For $  Ax = b + e  $, what we know from LS is that $  \hat{x}_{LS} = \arg \min_x \| Ax - b \|_2^2  $

Rewrite the equation into $  b = Ax + e  $,

While $  e \sim N(e | 0, \sigma^2 I)  $ (white Gaussian | 独立同方差)

$$
p(b | Ax) = N(b | Ax, \sigma^2 I) \\
= \frac{1}{Z} \exp \left[ -\frac{(b - Ax)^T (b - Ax)}{\sigma^2} \right]
$$

$$
\ln p(b | Ax) = \ln \frac{1}{Z} - \frac{(b - Ax)^T (b - Ax)}{\sigma^2} \\
\max_x \ln p(b | Ax) \Leftrightarrow \min_x \| Ax - b \|_2^2
$$

这样我们就把优化问题和概率问题联系起来，那这样联系起来有什么好处呢？如此就可以使用后验来对优化问题的解的置信区间进行描述。下面介绍如何描述

> $$\Rightarrow \text{后验 posterior: } p(\theta | y) = \frac{p(y | f_\theta(x)) \cdot p(\theta)}{p(y)}$$
>
>
> $$\Rightarrow \text{先验 prior: } p(\theta)$$

再次回到普通最小二乘的题设

$$
y = Ax + e, p(e) = N(0, I)
$$

$$
\begin{cases}p(y | Ax) = N(y | Ax, I) \\p(x) = N(x | 0, \gamma I), \gamma \to \infty\end{cases}
$$

$$
\Downarrow
$$

$$
p(x | y) = N(x | \mu, \Sigma), \text{where}
$$

$$
\begin{cases}\Sigma = \left( \frac{1}{\gamma} I + A^T A \right)^{-1} \quad \text{测量置信度} \\\mu = \left( \frac{1}{\gamma} I + A^T A \right)^{-1} A^T y\end{cases}
$$

于是我们可以根据方差测量置信区间

## 总体最小二乘

总体最小二乘Total Least Squares,TLS

在许多实际应用中，我们要求解

$$
Ax = b
$$
但由于测量误差或数据噪声，实际观测到的矩阵 $  A  $ 和向量 $  b  $ 都可能包含误差。

$$
A = A_0 + E, \; b = b_0 + e
$$
总体最小二乘法提供了一种在 $  A  $ 和 $  b  $ 都存在误差的情况下求解方程的方法。

$$
b + \Delta b = b_0 + e + \Delta b \to b_0
\\
A + \Delta A = A_0 + E + \Delta A \to A_0
\\
(A + \Delta A)x = b + \Delta b \to A_0 x = b_0
$$
总体最小二乘法目标：

找到 $  x  $，使得对 $  A, b  $ 的最小调整后，修正的方程精确成立，同时最小化校正数据矩阵 $  \Delta A  $ 和校正数据向量 $  \Delta b  $：

$$
\min_{\Delta A, \Delta b, x} \|\Delta A\|_F^2 + \|\Delta b\|_2^2 = \|\Delta A, \Delta b\|_F^2 \\
\text{Subject to } (A + \Delta A)x = b + \Delta b
$$
约束条件改写为

$$
([A, b] + [\Delta A, \Delta b]) \begin{bmatrix} x \\ -1 \end{bmatrix} = 0
$$
令$$B \triangleq [A, b], \; Z \triangleq \begin{bmatrix} x \\ -1 \end{bmatrix}, \; D \triangleq [\Delta A, \Delta b]$$

约束优化问题转化为
$$
\min_{D, Z} \|D\|_F^2 \\
\text{subject to } (B + D)Z = 0
$$
和 DLS 的约束优化问题对比：

$$
\min_{\Delta A, x} \|\Delta A\|_F^2 \\
\text{subject to } (\Delta A + A)x = b
$$

数据最小二乘解如下

$$
\hat{x}_{DLS} = \arg \min_x \frac{(Ax - b)^H (Ax - b)}{x^H x}
$$
TLS 解向量：

$$
\begin{bmatrix} x \\ -1 \end{bmatrix}_{TLS} = \arg \min_Z \frac{(BZ - 0)^H (BZ - 0)}{Z^H Z} = \arg \min_Z \frac{Z^H B^H BZ}{Z^H Z}
$$

## 约束总体最小二乘

## 盲矩阵方程求解的子空间方法

## 非负矩阵分解的优化理论

## 非负矩阵分解算法

## 系数矩阵方程求解：优化理论

## 系数矩阵方程求解：优化算法
